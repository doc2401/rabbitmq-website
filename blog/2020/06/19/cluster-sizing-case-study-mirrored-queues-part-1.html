<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">Cluster Sizing Case Study - Mirrored Queues Part 1 | RabbitMQ</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://www.rabbitmq.com/rabbitmq-website/img/rabbitmq-social-media-card.svg"><meta data-rh="true" name="twitter:image" content="https://www.rabbitmq.com/rabbitmq-website/img/rabbitmq-social-media-card.svg"><meta data-rh="true" property="og:url" content="https://www.rabbitmq.com/rabbitmq-website/blog/2020/06/19/cluster-sizing-case-study-mirrored-queues-part-1"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Cluster Sizing Case Study - Mirrored Queues Part 1 | RabbitMQ"><meta data-rh="true" name="description" content="In a first post in this sizing series we covered the workload, cluster and storage volume configurations on AWS ec2. In this post we’ll run a sizing analysis with mirrored queues."><meta data-rh="true" property="og:description" content="In a first post in this sizing series we covered the workload, cluster and storage volume configurations on AWS ec2. In this post we’ll run a sizing analysis with mirrored queues."><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2020-06-19T00:00:00.000Z"><meta data-rh="true" property="article:tag" content="Performance,Capacity Planning"><link data-rh="true" rel="icon" href="/rabbitmq-website/img/rabbitmq-logo.svg"><link data-rh="true" rel="canonical" href="https://www.rabbitmq.com/rabbitmq-website/blog/2020/06/19/cluster-sizing-case-study-mirrored-queues-part-1"><link data-rh="true" rel="alternate" href="https://www.rabbitmq.com/rabbitmq-website/blog/2020/06/19/cluster-sizing-case-study-mirrored-queues-part-1" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.rabbitmq.com/rabbitmq-website/blog/2020/06/19/cluster-sizing-case-study-mirrored-queues-part-1" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://H10VQIW16Y-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://www.rabbitmq.com/rabbitmq-website/blog/2020/06/19/cluster-sizing-case-study-mirrored-queues-part-1","mainEntityOfPage":"https://www.rabbitmq.com/rabbitmq-website/blog/2020/06/19/cluster-sizing-case-study-mirrored-queues-part-1","url":"https://www.rabbitmq.com/rabbitmq-website/blog/2020/06/19/cluster-sizing-case-study-mirrored-queues-part-1","headline":"Cluster Sizing Case Study - Mirrored Queues Part 1","name":"Cluster Sizing Case Study - Mirrored Queues Part 1","description":"In a first post in this sizing series we covered the workload, cluster and storage volume configurations on AWS ec2. In this post we’ll run a sizing analysis with mirrored queues.","datePublished":"2020-06-19T00:00:00.000Z","author":{"@type":"Person","name":"Jack Vanlightly"},"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://www.rabbitmq.com/rabbitmq-website/blog","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/rabbitmq-website/blog/rss.xml" title="RabbitMQ RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/rabbitmq-website/blog/atom.xml" title="RabbitMQ Atom Feed">




<link rel="search" type="application/opensearchdescription+xml" title="RabbitMQ" href="/rabbitmq-website/opensearch.xml">







<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway:400,700">
<script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-domain-script="018ee308-473e-754f-b0c2-cbe82d25512f"></script>
<script>function OptanonWrapper(){}</script>
<script>function setGTM(e,t,o,n,r){e[n]=e[n]||[],e[n].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var i=t.getElementsByTagName(o)[0],a=t.createElement(o),s="dataLayer"!=n?"&l="+n:"";a.async=!0,a.src="https://www.googletagmanager.com/gtm.js?id="+r+s,i.parentNode.insertBefore(a,i)}var timer;function waitForOnetrustActiveGroups(){document.cookie.indexOf("OptanonConsent")>-1&&document.cookie.indexOf("groups=")>-1?(clearTimeout(timer),setGTM(window,document,"script","dataLayer","GTM-TT84L8K")):timer=setTimeout(waitForOnetrustActiveGroups,250)}document.cookie.indexOf("OptanonConsent")>-1&&document.cookie.indexOf("groups=")>-1?setGTM(window,document,"script","dataLayer","GTM-TT84L8K"):waitForOnetrustActiveGroups()</script><link rel="stylesheet" href="/rabbitmq-website/styles.css">
<script src="/rabbitmq-website/runtime~main.js" defer="defer"></script>
<script src="/rabbitmq-website/main.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();null!==e?t(e):window.matchMedia("(prefers-color-scheme: dark)").matches?t("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,t("light"))}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><div class="announcementBar_mb4j" style="background-color:var(--ifm-color-primary-contrast-background);color:var(--ifm-font-color-base)" role="banner"><div class="announcementBarPlaceholder_vyr4"></div><div class="content_knG7 announcementBarContent_xLdY"><strong style="font-size: var(--ifm-h4-font-size);"><a href="https://github.com/rabbitmq/rabbitmq-server/releases/tag/v4.1.0">RabbitMQ 4.1.0 is out</a></strong></div><button type="button" aria-label="Close" class="clean-btn close closeButton_CVFx announcementBarClose_gvF7"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/rabbitmq-website/"><div class="navbar__logo"><img src="/rabbitmq-website/img/rabbitmq-logo-with-name.svg" alt="RabbitMQ" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/rabbitmq-website/img/rabbitmq-logo-with-name.svg" alt="RabbitMQ" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><a class="navbar__item navbar__link" href="/rabbitmq-website/tutorials">Getting Started</a><a class="navbar__item navbar__link" href="/rabbitmq-website/docs">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/rabbitmq-website/blog">Blog</a><a class="navbar__item navbar__link" href="/rabbitmq-website/contact">Support</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a class="navbar__link" aria-haspopup="true" aria-expanded="false" role="button" href="/rabbitmq-website/docs">4.1</a><ul class="dropdown__menu"><li class=""><strong>Release series</strong></li><li><a class="dropdown__link" href="/rabbitmq-website/docs/next">Next</a></li><li><a class="dropdown__link" href="/rabbitmq-website/docs">4.1</a></li><li><a class="dropdown__link" href="/rabbitmq-website/docs/4.0">4.0</a></li><li><a class="dropdown__link" href="/rabbitmq-website/docs/3.13">3.13</a></li><li><a href="https://v3-12.rabbitmq.com/documentation.html" target="_blank" rel="noopener noreferrer" class="dropdown__link">3.12<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a class="dropdown__link" href="/rabbitmq-website/release-information">Release Information</a></li></ul></div><a href="https://github.com/rabbitmq/rabbitmq-website" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><main class="col col--9 col--offset-1"><article class=""><header><h1 class="title_f1Hy">Cluster Sizing Case Study - Mirrored Queues Part 1</h1><div class="container_mt6G margin-vert--md"><time datetime="2020-06-19T00:00:00.000Z">June 19, 2020</time> · <!-- -->13 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><span class="authorName_yefp">Jack Vanlightly</span></div><div class="authorSocials_rSDt"></div></div></div></div></div></header><div id="__blog-post-container" class="markdown"><p>In a <a href="/rabbitmq-website/blog/2020/06/18/cluster-sizing-and-other-considerations">first post</a> in this sizing series we covered the workload, cluster and storage volume configurations on AWS ec2. In this post we’ll run a sizing analysis with mirrored queues.</p>
<p>The first phase of our sizing analysis will be assessing what intensities each of our clusters and storage volumes can handle easily and which are too much.</p>
<p>All tests use the following policy:</p>
<ul>
<li>ha-mode: exactly</li>
<li>ha-params: 2</li>
<li>ha-sync-mode: manual</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="ideal-conditions--growing-intensity-tests">Ideal Conditions - Growing Intensity Tests<a href="#ideal-conditions--growing-intensity-tests" class="hash-link" aria-label="Direct link to Ideal Conditions - Growing Intensity Tests" title="Direct link to Ideal Conditions - Growing Intensity Tests">​</a></h2>
<p>In a previous <a href="/rabbitmq-website/blog/2020/06/04/how-to-run-benchmarks">post</a> we discussed options for running benchmarks. You can run this workload, at these intensities with the following command:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">bin/runjava com.rabbitmq.perf.PerfTest \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-H amqp://guest:guest@10.0.0.1:5672/%2f,amqp://guest:guest@10.0.0.2:5672/%2f,amqp://guest:guest@10.0.0.3:5672/%2f \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-z 1800 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-f persistent \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-q 1000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-c 1000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-ct -1 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--rate 50 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--size 1024 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--queue-pattern &#x27;perf-test-%d&#x27; \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--queue-pattern-from 1 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--queue-pattern-to 100 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--producers 200 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--consumers 200 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--consumer-latency 10000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--producer-random-start-delay 30</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Just change the --rate argument to the rate you need for each test and remember that it is the rate per publisher rather than the total combined rate. Because the consumer processing time (consumer latency) is set to 10ms, we also need to increase the number of consumers for the higher publish rates.</p>
<p>Before running PerfTest you will need to create a policy to turn the created queues into mirrored queues with one master and the number of mirrors you wish to test with.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="io1---high-performance-ssd">io1 - High Performance SSD<a href="#io1---high-performance-ssd" class="hash-link" aria-label="Direct link to io1 - High Performance SSD" title="Direct link to io1 - High Performance SSD">​</a></h3>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 1. Increasing intensity tests and the io1 volume" src="/rabbitmq-website/assets/images/mirrored-io1-phase1-throughput-1-509aac7e3029b8d1699ff4f58948f305.png" width="909" height="394" class="img_ev3q"><figcaption>Fig 1. Increasing intensity tests and the io1 volume</figcaption></figure><p></p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 2. End-to-end latencies for clusters 3x36, 3x16, 5x16 and 7x16." src="/rabbitmq-website/assets/images/mirrored-io1-phase1-latency1-1af4dd09e20fca48f7962e32e00d2ee7.png" width="915" height="556" class="img_ev3q"><figcaption>Fig 2. End-to-end latencies for clusters 3x36, 3x16, 5x16 and 7x16.</figcaption></figure><p></p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 3. End-to-end latencies for clusters 5x8, 7x8, and 9x8. Note that the 99th percentile for the 5x8 reached 30 seconds." src="/rabbitmq-website/assets/images/mirrored-io1-phase1-latency2-cb02bb3a91ab081f1bfe7ed8fc9b59d7.png" width="917" height="558" class="img_ev3q"><figcaption>Fig 3. End-to-end latencies for clusters 5x8, 7x8, and 9x8. Note that the 99th percentile for the 5x8 reached 30 seconds.</figcaption></figure><p></p>
<p>Different clusters managed to reach different intensities but all tests except for one remained under the 1 second for 99th percentile end-to-end latency requirement.</p>
<p>The limiting factor in most tests was CPU so no surprise that the configuration with most CPUs did best. What was interesting is that the 3 node, 36 vCPU cluster (3x36) that had the second highest total vCPU count was way down, below the 7x8 vCPU cluster. In fact, CPU didn’t go above 50% on the 3x36 cluster, it seems that Erlang was not able to efficiently make use of all those 36 vCPUs per broker (more on that later).</p>
<p>Disk throughput did not get close to capacity, but IOPS did reach 8000-9000 in all configurations with roughly 5-7kb write sizes. Network bandwidth remained lower than 1gbit, which was within the limit of all VMs.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bottom-throughput-cluster-3x16">Bottom throughput cluster (3x16):<a href="#bottom-throughput-cluster-3x16" class="hash-link" aria-label="Direct link to Bottom throughput cluster (3x16):" title="Direct link to Bottom throughput cluster (3x16):">​</a></h3>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 4. CPU and Network for the poorest performing cluster (3x16)." src="/rabbitmq-website/assets/images/mirrored-io1-phase1-bottom-cpu-network-d983fa319f267932caea29d328224fa1.png" width="916" height="393" class="img_ev3q"><figcaption>Fig 4. CPU and Network for the poorest performing cluster (3x16).</figcaption></figure><p></p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 5. Disk usage for the poorest performing cluster (3x16)." src="/rabbitmq-website/assets/images/mirrored-io1-phase1-bottom-disk-1024x635-e1cdb53c93766f8d8d1cad7ce296a7d8.png" width="1024" height="635" class="img_ev3q"><figcaption>Fig 5. Disk usage for the poorest performing cluster (3x16).</figcaption></figure><p></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="top-throughput-cluster-7x16">Top throughput cluster (7x16)<a href="#top-throughput-cluster-7x16" class="hash-link" aria-label="Direct link to Top throughput cluster (7x16)" title="Direct link to Top throughput cluster (7x16)">​</a></h3>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 6. CPU and network for the top performing cluster (7x16)." src="/rabbitmq-website/assets/images/mirrored-io1-phase1-top-cpu-network-87e5ad035c7c8e186483706f834e8632.png" width="911" height="393" class="img_ev3q"><figcaption>Fig 6. CPU and network for the top performing cluster (7x16).</figcaption></figure><p></p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 7. Disk usage for the top performing cluster (7x16)." src="/rabbitmq-website/assets/images/mirrored-io1-phase1-top-disk-1024x634-6107c124110624389c1ac2911f47108b.png" width="1024" height="634" class="img_ev3q"><figcaption>Fig 7. Disk usage for the top performing cluster (7x16).</figcaption></figure><p></p>
<blockquote>
<p>Insight: The lower performing 3x16 cluster had a slightly higher disk write throughput than the top performing 7x16 cluster. This is because a message is not persisted to disk if it has already been consumed by the time an fsync is performed. So when consumers keep up, disk writes are lower. If we look at the 25k msg/s test which is the highest both can handle, we see that the 3x16 cluster has a 95th percentile latency of around 250ms and the 7x16 around 4ms. Fsyncs occur every 200ms roughly. So the higher performing cluster performs less writes.</p>
</blockquote>
<p><strong>Leaderboard in matched target throughput</strong></p>
<p>The highest throughput each cluster size managed, where it delivered exactly the target rate.</p>
<ol>
<li>Cluster: 7 nodes, 16 vCPUs (c5.4xlarge). Rate: 65k msg/s</li>
<li>Cluster: 5 nodes, 16 vCPUs (c5.4xlarge). Rate: 45k msg/s</li>
<li>Cluster: 9 nodes, 8 vCPUs ( c5.2xlarge). Rate: 45k msg/s</li>
<li>Cluster: 7 nodes, 8 vCPUs (c5.2xlarge). Rate: 40k msg/s</li>
<li>Cluster: 3 nodes, 36 vCPUs (c5.9xlarge). Rate: 30k msg/s</li>
<li>Cluster: 5 nodes, 8 vCPUs (c5.2xlarge). Rate: 20k msg/s</li>
<li>Cluster: 3 nodes, 16 vCPUs (c5.4xlarge). Rate: 20k msg/s</li>
</ol>
<p>We see the middle ground of scaling up and out gave us the best throughput but scaling out did pretty well too. Scaling up was not so good.</p>
<p><strong>Leaderboard in cost per 1000 mgs/s per month, at top throughput.</strong></p>
<ol>
<li>Cluster: 5 nodes, 16 vCPUs. Cost: $134 (45k msg/s)</li>
<li>Cluster: 7 nodes, 16 vCPUs. Cost: $140 (65k msg/s)</li>
<li>Cluster: 7 nodes, 8 vCPUs. Cost: $170 (40k msg/s)</li>
<li>Cluster: 3 nodes, 16 vCPUs. Cost: $182 (20k msg/s)</li>
<li>Cluster: 3 nodes, 36 vCPUs. Cost: $182 (30k msg/s)</li>
<li>Cluster: 9 nodes, 8 vCPUs. Cost: $194 (45k msg/s)</li>
<li>Cluster: 5 nodes, 8 vCPUs. Cost: $242 (20k msg/s)</li>
</ol>
<p><strong>Leaderboard in cost per 1000 msg/s per month at a target of 30k msg/s.</strong></p>
<ol>
<li>Cluster: 3 nodes, 36 vCPUs. Cost: $183</li>
<li>Cluster: 5 nodes, 16 vCPUs. Cost: $202</li>
<li>Cluster: 7 nodes, 8 vCPUs. Cost: $226</li>
<li>Cluster: 9 nodes, 8 vCPUs. Cost: $291</li>
<li>Cluster: 7 nodes, 16 vCPUs. Cost: $307</li>
</ol>
<p>While scaling out with smaller VMs had decent throughput, it was undermined by the fact that our disks were the most expensive item. Because the disks were expensive, the most cost effective cluster was the one with the least instances, however, this 3x36 cluster only just managed the peak in this ideal conditions test, it is unlikely to hold up under the more difficult tests. So if we ignore the 3x36 cluster, the most cost effective was the middle ground of scaling up and out.</p>
<p>Do we really need those costly io1 SSDs? We’re not needing them for IO throughput, but those 10000 IOPS are almost fully utilised. Will the 3000 IOP gp2 handle the higher intensities?</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="gp2---general-purpose-ssd">gp2 - General Purpose SSD<a href="#gp2---general-purpose-ssd" class="hash-link" aria-label="Direct link to gp2 - General Purpose SSD" title="Direct link to gp2 - General Purpose SSD">​</a></h3>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 8. Increasing intensity test with the gp2 volume." src="/rabbitmq-website/assets/images/mirrored-gp2-phase1-throughput-cfd703dd660441fc0bbc0594698c748d.png" width="907" height="390" class="img_ev3q"><figcaption>Fig 8. Increasing intensity test with the gp2 volume.</figcaption></figure><p></p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 9. End-to-end latencies for clusters 3x36, 3x16, 5x16 and 7x16." src="/rabbitmq-website/assets/images/mirrored-gp2-phase1-latency1-561a0dfceb20b6a3c043dd3ff7e3321d.png" width="910" height="553" class="img_ev3q"><figcaption>Fig 9. End-to-end latencies for clusters 3x36, 3x16, 5x16 and 7x16.</figcaption></figure><p></p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 10. End-to-end latencies for clusters 5x8, 7x8, and 9x8." src="/rabbitmq-website/assets/images/mirrored-gp2-phase1-latency2-741952eccb11cde3227a0d675e13d694.png" width="911" height="551" class="img_ev3q"><figcaption>Fig 10. End-to-end latencies for clusters 5x8, 7x8, and 9x8.</figcaption></figure><p></p>
<p>Again, different clusters managed different throughputs, but all came in within the end-to-end latency requirements.</p>
<p>We have a new winner with the gp2 volume: the 9x8 cluster followed by the 7x8 cluster. The 7x16 and 5x16 showed some choppy throughput once they had reached and then past their top capacity. The bottom two were the same: the 3x36 and 3x16 clusters.</p>
<p>So how did RabbitMQ handle the lower IOPs volumes?</p>
<p><strong>Bottom Throughput Cluster (3x16)</strong></p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 11. Disk usage for the poorest performing cluster (3x16)." src="/rabbitmq-website/assets/images/mirrored-gp2-phase1-bottom-disk-1024x635-1a65faf65127acd3c3aeae2a0f05dbc6.png" width="1024" height="635" class="img_ev3q"><figcaption>Fig 11. Disk usage for the poorest performing cluster (3x16).</figcaption></figure><p></p>
<p>We hit the 3000 IOPs limit from the 5000 msg/s rate and upward. As the tests go on, in order to cope with the larger disk throughput, the IO size increases. So it looks like we don’t need all those IOPs after all.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="top-throughput-cluster-9x8">Top Throughput Cluster (9x8)<a href="#top-throughput-cluster-9x8" class="hash-link" aria-label="Direct link to Top Throughput Cluster (9x8)" title="Direct link to Top Throughput Cluster (9x8)">​</a></h3>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 12. Disk usage for the top performing cluster (9x8)." src="/rabbitmq-website/assets/images/mirrored-gp2-phase1-top-disk-1024x612-a11ac3d5f3f3fb4da6fcca2ac68e60a3.png" width="1024" height="612" class="img_ev3q"><figcaption>Fig 12. Disk usage for the top performing cluster (9x8).</figcaption></figure><p></p>
<p>Exactly the same story for the highest performing cluster - RabbitMQ adjusted to the lower available IOPs.</p>
<p><strong>Leaderboard in matched target throughput</strong></p>
<ol>
<li>Cluster: 9 nodes, 8 vCPUs (c5.2xlarge). Rate: 65k msg/s</li>
<li>Cluster: 7 nodes, 8 vCPUs (c5.2xlarge). Rate: 50k msg/s</li>
<li>Cluster: 7 nodes, 16 vCPUs (c5.4xlarge). Rate: 50k msg/s</li>
<li>Cluster: 5 nodes, 16 vCPUs (c5.4xlarge). Rate: 40k msg/s</li>
<li>Cluster: 3 nodes, 36 vCPUs (c5.9xlarge). Rate: 35k msg/s</li>
<li>Cluster: 5 nodes, 8 vCPUs (c5.2xlarge). Rate: 35k msg/s</li>
<li>Cluster: 3 nodes, 16 vCPUs (c5.4xlarge). Rate: 25k msg/s</li>
</ol>
<p>This time, scaling out was clearly the most effective. Scaling up was not so good.</p>
<p><strong>Leaderboard in cost per 1000 messages per month, at top throughput.</strong></p>
<ol>
<li>Cluster: 9 nodes, 8 vCPUs. Cost: $48 (65k msg/s)</li>
<li>Cluster: 7 nodes, 8 vCPUs. Cost: $48 (50k msg/s)</li>
<li>Cluster: 5 nodes, 8 vCPUs. Cost: $49 (35k msg/s)</li>
<li>Cluster: 3 nodes, 16 vCPUs. Cost: $71 (25k msg/s)</li>
<li>Cluster: 5 nodes, 16 vCPUs. Cost: $74 (40k msg/s)</li>
<li>Cluster: 7 nodes, 16 vCPUs. Cost: $96 (50k msg/s)</li>
<li>Cluster: 3 nodes, 36 vCPUs. Cost: $103 (35k msg/s)</li>
</ol>
<p>In terms of cost per top throughput the order was: <em>core count desc, node count desc</em>.</p>
<p><strong>Leaderboard in cost per 1000 messages per month at a target of 30k msg/s.</strong></p>
<ol>
<li>Cluster: 5 nodes, 8 vCPUs. Cost: $58</li>
<li>Cluster: 7 nodes, 8 vCPUs. Cost: $81</li>
<li>Cluster: 5 nodes, 16 vCPUs. Cost: $98</li>
<li>Cluster: 9 nodes, 8 vCPUs. Cost: $104</li>
<li>Cluster: 3 nodes, 36 vCPUs. Cost: $120</li>
<li>Cluster: 7 nodes, 16 vCPUs. Cost: $161</li>
</ol>
<p>At our target rate of 30k msg/s, the 5x8 was the best. The conclusion is that scaling out smaller VMs gave both better performance but also best cost effectiveness. The reason was that the gp2 volumes are relatively cheap and we don’t get penalised for scaling out like we did with the expensive io1 volumes.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="st1---hdd">st1 - HDD<a href="#st1---hdd" class="hash-link" aria-label="Direct link to st1 - HDD" title="Direct link to st1 - HDD">​</a></h3>
<p>So far we’ve seen RabbitMQ make use of the available IOPs it has available by adjusting to lower IOPs as needed. HDDs are built for large and sequential workloads with low IOPs so will RabbitMQ be able to adjust its disk operations yet again to make even fewer, but larger disk operations? Let’s see.</p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 13. Increasing intensity test with the st1 volume." src="/rabbitmq-website/assets/images/mirrored-st1-phase1-throughput-19e01fb0205a446f38366e1668a4ae8d.png" width="913" height="392" class="img_ev3q"><figcaption>Fig 13. Increasing intensity test with the st1 volume.</figcaption></figure><p></p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 14. End-to-end latencies for clusters 3x36, 3x16, 5x16 and 7x16." src="/rabbitmq-website/assets/images/mirrored-st1-phase1-latency1-cc78f47ef4c5f0be8b19734889d4eaea.png" width="911" height="556" class="img_ev3q"><figcaption>Fig 14. End-to-end latencies for clusters 3x36, 3x16, 5x16 and 7x16.</figcaption></figure><p></p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 15. End-to-end latencies for clusters 5x8, 7x8 and 9x8." src="/rabbitmq-website/assets/images/mirrored-st1-phase1-latency2-2f1d100ba8a64bf688ace5e581975238.png" width="925" height="556" class="img_ev3q"><figcaption>Fig 15. End-to-end latencies for clusters 5x8, 7x8 and 9x8.</figcaption></figure><p></p>
<p>Looking at the throughput, HDDs did the best of all in terms of top throughput, actually achieving the 70k msg/s highest intensity. The end-to-end latency had a different pattern. With the SSDs the latencies started out very small and then grew as the intensity grew. But the HDDs started out with a much higher latency and it grew at a much lower rate.</p>
<p>Let’s see how RabbitMQ used the available IOPs.</p>
<p><strong>Bottom Throughput Cluster (3x16)</strong></p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 16. Disk usage for the poorest performing cluster (3x16)." src="/rabbitmq-website/assets/images/mirrored-st1-phase1-bottom-disk-1024x636-d3ee291c995b6c6d15a8a4994bd7d2ad.png" width="1024" height="636" class="img_ev3q"><figcaption>Fig 16. Disk usage for the poorest performing cluster (3x16).</figcaption></figure><p></p>
<p>We’re down now to 500-600 IOPS with an average write size approaching 50kb (ignoring the initial peaks). Far fewer, but much larger IO operations. IO latency is much higher. We’ve gone from 0.5ms (io1), 2ms (gp2) to 10ms (st1) which is likely a combination of the IO size and ability of the storage drive to do random IO fast. Further down, we’ll compare end-to-end latency for the three disk types.</p>
<p><strong>Top Throughput Cluster (9x8)</strong></p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 17. Disk usage for the top performing cluster (9x8)." src="/rabbitmq-website/assets/images/mirrored-st1-phase1-top-disk-1024x686-3f44359cc747ddb05548345cfdc21f51.png" width="1024" height="686" class="img_ev3q"><figcaption>Fig 17. Disk usage for the top performing cluster (9x8).</figcaption></figure><p></p>
<p><strong>Leaderboard in throughput</strong></p>
<p>Only three sizes were able to meet their targets, the rest were always short, even as their throughput climbed in each test. We might have been able to increase throughput with different publisher confirm in-flight limits, a higher limit might have benefited the higher latency HDDs.</p>
<p>The winners that were able to hit their target were:</p>
<ol>
<li>Cluster: 9 nodes, 8 vCPUs (c5.2xlarge). Rate: 70k msg/s</li>
<li>Cluster: 7 nodes, 16 vCPUs (c5.4xlarge). Rate: 65k msg/s</li>
<li>Cluster: 7 nodes, 8 vCPUs (c5.2xlarge). Rate: 55k msg/s</li>
</ol>
<p>The rest fell short of their targets in each test, but still showed higher throughput as the tests progressed:</p>
<ol>
<li>Cluster: 5 nodes, 16 vCPUs (c5.4xlarge). Rate: 55k msg/s</li>
<li>Cluster: 5 nodes, 8 vCPUs (c5.2xlarge). Rate: 43k msg/s</li>
<li>Cluster: 3 nodes, 36 vCPUs (c5.9xlarge). Rate: 35k msg/s</li>
<li>Cluster: 3 nodes, 16 vCPUs (c5.4xlarge). Rate: 29k msg/s</li>
</ol>
<p>With HDDs, it was all about scaling out, not up. </p>
<p><strong>Leaderboard in cost per 1000 messages per month, at their top rate.</strong></p>
<ol>
<li>Cluster: 5 nodes, 8 vCPUs. Cost: $65 (43k msg/s)</li>
<li>Cluster: 7 nodes, 8 vCPUs. Cost: $71 (55k msg/s)</li>
<li>Cluster: 9 nodes, 8 vCPUs. Cost: $72 (70k msg/s)</li>
<li>Cluster: 5 nodes, 16 vCPUs. Cost: $73 (55k msg/s)</li>
<li>Cluster: 3 nodes, 16 vCPUs. Cost: $83 (29k msg/s)</li>
<li>Cluster: 7 nodes, 16 vCPUs. Cost: $97 (65k msg/s)</li>
<li>Cluster: 3 nodes, 36 vCPUs. Cost: $121 (35k msg/s)</li>
</ol>
<p><strong>Leaderboard in cost per 1000 messages per month at a target of 30k msg/s.</strong></p>
<ol>
<li>Cluster: 5 nodes, 8 vCPUs. Cost: $93</li>
<li>Cluster: 7 nodes, 8 vCPUs. Cost: $131</li>
<li>Cluster: 5 nodes, 16 vCPUs. Cost: $134</li>
<li>Cluster: 3 nodes, 36 vCPUs. Cost: $142</li>
<li>Cluster: 9 nodes, 8 vCPUs. Cost: $168</li>
<li>Cluster: 7 nodes, 16 vCPUs. Cost: $211</li>
</ol>
<p>In terms of their top throughput, its clear that smaller VMs were most cost effective. But when considering the target of 30k msg/s, the middle ground of scaling out/up showed the best results. There is some conflict again between scaling out which is best for performance and cost (the st1 volumes are a little costly). So the middle ground wins.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-latency-and-the-three-volume-types">End-to-end Latency and the Three Volume Types<a href="#end-to-end-latency-and-the-three-volume-types" class="hash-link" aria-label="Direct link to End-to-end Latency and the Three Volume Types" title="Direct link to End-to-end Latency and the Three Volume Types">​</a></h3>
<p>In these tests we treat end-to-end latency as the time between a message being published and consumed. If we look at the 30k msg/s target rate and the 7x16 cluster type we see:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="io1">io1<a href="#io1" class="hash-link" aria-label="Direct link to io1" title="Direct link to io1">​</a></h3>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 18. 50th and 75th percentile latencies for io1 at 30k msg/s." src="/rabbitmq-website/assets/images/latency-50-75-io1-7x16-30k-00342baadc10ec90a8377e0d3ee7360b.png" width="911" height="232" class="img_ev3q"><figcaption>Fig 18. 50th and 75th percentile latencies for io1 at 30k msg/s.</figcaption></figure><p></p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 19. 50th, 75th, 95th, 99th and 99.9th percentile latencies for io1 at 30k msg/s." src="/rabbitmq-website/assets/images/latency-all-io1-7x16-30k-29179fa6782a94956f502a7349f92a5f.png" width="909" height="232" class="img_ev3q"><figcaption>Fig 19. 50th, 75th, 95th, 99th and 99.9th percentile latencies for io1 at 30k msg/s.</figcaption></figure><p></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="gp2">gp2<a href="#gp2" class="hash-link" aria-label="Direct link to gp2" title="Direct link to gp2">​</a></h3>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 20. 50th and 75th percentile latencies for gp2 at 30k msg/s." src="/rabbitmq-website/assets/images/latency-50-75-gp2-7x16-30k-b7d2110d390914edd7b3958d0f000f06.png" width="909" height="229" class="img_ev3q"><figcaption>Fig 20. 50th and 75th percentile latencies for gp2 at 30k msg/s.</figcaption></figure><p></p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 21. 50th, 75th, 95th, 99th and 99.9th percentile latencies for gp2 at 30k msg/s." src="/rabbitmq-website/assets/images/latency-all-gp2-7x16-30k-0b15a97df58b5e89872946d77c216b22.png" width="909" height="230" class="img_ev3q"><figcaption>Fig 21. 50th, 75th, 95th, 99th and 99.9th percentile latencies for gp2 at 30k msg/s.</figcaption></figure><p></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="st1">st1<a href="#st1" class="hash-link" aria-label="Direct link to st1" title="Direct link to st1">​</a></h3>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 22. 50th and 75th percentile latencies for st1 at 30k msg/s." src="/rabbitmq-website/assets/images/latency-50-75-st1-7x16-30k-6486c51be133c9a720ed423634688dfb.png" width="910" height="230" class="img_ev3q"><figcaption>Fig 22. 50th and 75th percentile latencies for st1 at 30k msg/s.</figcaption></figure><p></p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 23. 50th, 75th, 95th, 99th and 99.9th percentile latencies for st1 at 30k msg/s." src="/rabbitmq-website/assets/images/latency-all-st1-7x16-30k-34183c7e86089c17cf9e37ad97dc4b23.png" width="909" height="226" class="img_ev3q"><figcaption>Fig 23. 50th, 75th, 95th, 99th and 99.9th percentile latencies for st1 at 30k msg/s.</figcaption></figure><p></p>
<p>We see that io1 delivers the best latencies up to 95th percentile, after that it matched more or less gp2. The st1 HDD showed much higher in latencies but still within our target of 1 second.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cpu-utilisation-and-36-vcpu-vms">CPU Utilisation and 36 vCPU VMs<a href="#cpu-utilisation-and-36-vcpu-vms" class="hash-link" aria-label="Direct link to CPU Utilisation and 36 vCPU VMs" title="Direct link to CPU Utilisation and 36 vCPU VMs">​</a></h3>
<p>In the 8 and 16 vCPU (CPU thread) instances, CPU seems to be the bottleneck. Once CPU reached above 90% we stopped seeing any more increases in throughput. However, the 36 vCPU instances tended to always look like this:</p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 24. CPU utilisation on the 3x36 cluster." src="/rabbitmq-website/assets/images/cpu-3x36-88dba6232fdc2d62bd3adb55abc03140.png" width="910" height="390" class="img_ev3q"><figcaption>Fig 24. CPU utilisation on the 3x36 cluster.</figcaption></figure><p></p>
<p>CPU utilisation and Erlang is not always straightforward. Erlang schedulers will do busy-waiting when they deem that it is more effective to do so. This means that rather than sleep, a scheduler sits in a busy loop, utilising CPU while it waits for new work to do. This can make it appear that the Erlang application is doing lots of work, using the CPU, but actually it is just waiting for work.</p>
<p>36 vCPUs is just a waste in this particular case. We’ve seen that they are more expensive and deliver inferior results compared to scaled out smaller VMs.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="increasing-intensity-benchmarks---conclusion">Increasing Intensity Benchmarks - Conclusion<a href="#increasing-intensity-benchmarks---conclusion" class="hash-link" aria-label="Direct link to Increasing Intensity Benchmarks - Conclusion" title="Direct link to Increasing Intensity Benchmarks - Conclusion">​</a></h2>
<p>So far the conclusions are:</p>
<ul>
<li>The expensive io1 is only worth it if we really care about end-to-end latency.</li>
<li>The inexpensive gp2 offers the best combination of performance and cost and is the best option for most workloads. Just remember that we used a 1TB size that does not have burst IOPs and there is a 250 MiBs limit (which we never hit).</li>
<li>With cheap storage volumes, scaling out the smaller 8 vCPU VMs was the most cost effective and best in terms of performance.</li>
<li>With expensive volumes, going with the middle ground of scaling out and up was most cost effective.</li>
<li>CPU utilisation seemed to be the limiting factor on the 16 and 8 vCPUs. The large 36 vCPU instances didn’t go past 60% utilisation, but also didn’t hit disk or network limits. Erlang just couldn’t efficiently use that many cores.</li>
</ul>
<p>Top 5 Configurations for cost per 1000 msg/s per month for the 30k msg/s throughput:</p>
<ol>
<li>Cluster: 5 nodes, 8 vCPUs, gp2 SDD. Cost: $58</li>
<li>Cluster: 7 nodes, 8 vCPUs, gp2 SDD. Cost: $81</li>
<li>Cluster: 5 nodes, 8 vCPUs, st1 HDD. Cost: $93</li>
<li>Cluster: 5 nodes, 16 vCPUs, gp2 SDD. Cost: $98</li>
<li>Cluster: 9 nodes, 8 vCPUs, gp2 SDD. Cost: $104</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="weve-only-tested-under-ideal-conditions">We&#x27;ve only tested under ideal conditions...<a href="#weve-only-tested-under-ideal-conditions" class="hash-link" aria-label="Direct link to We&#x27;ve only tested under ideal conditions..." title="Direct link to We&#x27;ve only tested under ideal conditions...">​</a></h2>
<p>We&#x27;ve gathered a lot of data from 21 different cluster configurations at 15 different workload intensities. We think that so far we should go with a medium to large cluster of small VMs on the inexpensive gp2 volumes. But this was testing the happy scenario where queues are empty or close to empty where RabbitMQ operates at its peak performance. Next we&#x27;ll run more tests that ensure that despite brokers being lost and queue backlogs occurring that our chosen cluster size continues to deliver the performance that we need. <a href="/rabbitmq-website/blog/2020/06/20/cluster-sizing-case-study-mirrored-queues-part-2">Next</a> we test resiliency.</p></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/rabbitmq-website/blog/tags/performance">Performance</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/rabbitmq-website/blog/tags/capacity-planning">Capacity Planning</a></li></ul></div></div><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><a href="https://github.com/rabbitmq/rabbitmq-website/tree/main/blog/2020-06-19-cluster-sizing-case-study-mirrored-queues-part-1/index.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/rabbitmq-website/blog/2020/06/20/cluster-sizing-case-study-mirrored-queues-part-2"><div class="pagination-nav__sublabel">Newer post</div><div class="pagination-nav__label">Cluster Sizing Case Study – Mirrored Queues Part 2</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/rabbitmq-website/blog/2020/06/18/cluster-sizing-and-other-considerations"><div class="pagination-nav__sublabel">Older post</div><div class="pagination-nav__label">Cluster Sizing and Other Considerations</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#ideal-conditions--growing-intensity-tests" class="table-of-contents__link toc-highlight">Ideal Conditions - Growing Intensity Tests</a><ul><li><a href="#io1---high-performance-ssd" class="table-of-contents__link toc-highlight">io1 - High Performance SSD</a></li><li><a href="#bottom-throughput-cluster-3x16" class="table-of-contents__link toc-highlight">Bottom throughput cluster (3x16):</a></li><li><a href="#top-throughput-cluster-7x16" class="table-of-contents__link toc-highlight">Top throughput cluster (7x16)</a></li><li><a href="#gp2---general-purpose-ssd" class="table-of-contents__link toc-highlight">gp2 - General Purpose SSD</a></li><li><a href="#top-throughput-cluster-9x8" class="table-of-contents__link toc-highlight">Top Throughput Cluster (9x8)</a></li><li><a href="#st1---hdd" class="table-of-contents__link toc-highlight">st1 - HDD</a></li><li><a href="#end-to-end-latency-and-the-three-volume-types" class="table-of-contents__link toc-highlight">End-to-end Latency and the Three Volume Types</a></li><li><a href="#io1" class="table-of-contents__link toc-highlight">io1</a></li><li><a href="#gp2" class="table-of-contents__link toc-highlight">gp2</a></li><li><a href="#st1" class="table-of-contents__link toc-highlight">st1</a></li><li><a href="#cpu-utilisation-and-36-vcpu-vms" class="table-of-contents__link toc-highlight">CPU Utilisation and 36 vCPU VMs</a></li></ul></li><li><a href="#increasing-intensity-benchmarks---conclusion" class="table-of-contents__link toc-highlight">Increasing Intensity Benchmarks - Conclusion</a></li><li><a href="#weve-only-tested-under-ideal-conditions" class="table-of-contents__link toc-highlight">We&#39;ve only tested under ideal conditions...</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Learn about RabbitMQ</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/rabbitmq-website/tutorials">Getting Started</a></li><li class="footer__item"><a class="footer__link-item" href="/rabbitmq-website/docs">Documentation</a></li><li class="footer__item"><a class="footer__link-item" href="/rabbitmq-website/blog">Blog</a></li></ul></div><div class="col footer__col"><div class="footer__title">Reach out to the RabbitMQ team</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/rabbitmq" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/rabbitmq/rabbitmq-server/discussions" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub Discussions<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a class="footer__link-item" href="/rabbitmq-website/contact?utm_source=rmq_release-information_tableheader&amp;utm_medium=rmq_website&amp;utm_campaign=tanzu">Long Term Commercial Support</a></li><li class="footer__item"><a class="footer__link-item" href="/rabbitmq-website/contact">Contact Us</a></li><li class="footer__item"><a href="https://www.rabbitmq.com/discord" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Broadcom</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://tanzu.vmware.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">VMware Tanzu<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.vmware.com/help/legal.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">Terms of Use<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.vmware.com/help/privacy.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">Privacy<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a class="footer__link-item" href="/rabbitmq-website/trademark-guidelines">Trademark Guidelines</a></li><li class="footer__item"><a href="https://www.vmware.com/help/privacy/california-privacy-rights.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">Your California Privacy Rights<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a class="footer__link-item ot-sdk-show-settings">Cookie Settings</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2005-2025 Broadcom. All Rights Reserved. The term "Broadcom" refers to Broadcom Inc. and/or its subsidiaries.</div></div></div></footer></div>
</body>
</html>