<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">Quorum Queues and Flow Control - Single Queue Benchmarks | RabbitMQ</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://www.rabbitmq.com/rabbitmq-website/img/rabbitmq-social-media-card.svg"><meta data-rh="true" name="twitter:image" content="https://www.rabbitmq.com/rabbitmq-website/img/rabbitmq-social-media-card.svg"><meta data-rh="true" property="og:url" content="https://www.rabbitmq.com/rabbitmq-website/blog/2020/05/14/quorum-queues-and-flow-control-single-queue-benchmarks"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Quorum Queues and Flow Control - Single Queue Benchmarks | RabbitMQ"><meta data-rh="true" name="description" content="In the last post we covered what flow control is, both as a general concept and the various flow control mechanisms available in RabbitMQ. We saw that publisher confirms and consumer acknowledgements are not just data safety measures, but also play a role in flow control."><meta data-rh="true" property="og:description" content="In the last post we covered what flow control is, both as a general concept and the various flow control mechanisms available in RabbitMQ. We saw that publisher confirms and consumer acknowledgements are not just data safety measures, but also play a role in flow control."><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2020-05-14T00:00:00.000Z"><meta data-rh="true" property="article:tag" content="Performance"><link data-rh="true" rel="icon" href="/rabbitmq-website/img/rabbitmq-logo.svg"><link data-rh="true" rel="canonical" href="https://www.rabbitmq.com/rabbitmq-website/blog/2020/05/14/quorum-queues-and-flow-control-single-queue-benchmarks"><link data-rh="true" rel="alternate" href="https://www.rabbitmq.com/rabbitmq-website/blog/2020/05/14/quorum-queues-and-flow-control-single-queue-benchmarks" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.rabbitmq.com/rabbitmq-website/blog/2020/05/14/quorum-queues-and-flow-control-single-queue-benchmarks" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://H10VQIW16Y-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://www.rabbitmq.com/rabbitmq-website/blog/2020/05/14/quorum-queues-and-flow-control-single-queue-benchmarks","mainEntityOfPage":"https://www.rabbitmq.com/rabbitmq-website/blog/2020/05/14/quorum-queues-and-flow-control-single-queue-benchmarks","url":"https://www.rabbitmq.com/rabbitmq-website/blog/2020/05/14/quorum-queues-and-flow-control-single-queue-benchmarks","headline":"Quorum Queues and Flow Control - Single Queue Benchmarks","name":"Quorum Queues and Flow Control - Single Queue Benchmarks","description":"In the last post we covered what flow control is, both as a general concept and the various flow control mechanisms available in RabbitMQ. We saw that publisher confirms and consumer acknowledgements are not just data safety measures, but also play a role in flow control.","datePublished":"2020-05-14T00:00:00.000Z","author":{"@type":"Person","name":"Jack Vanlightly"},"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://www.rabbitmq.com/rabbitmq-website/blog","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/rabbitmq-website/blog/rss.xml" title="RabbitMQ RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/rabbitmq-website/blog/atom.xml" title="RabbitMQ Atom Feed">




<link rel="search" type="application/opensearchdescription+xml" title="RabbitMQ" href="/rabbitmq-website/opensearch.xml">







<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway:400,700">
<script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-domain-script="018ee308-473e-754f-b0c2-cbe82d25512f"></script>
<script>function OptanonWrapper(){}</script>
<script>function setGTM(e,t,o,n,r){e[n]=e[n]||[],e[n].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var i=t.getElementsByTagName(o)[0],a=t.createElement(o),s="dataLayer"!=n?"&l="+n:"";a.async=!0,a.src="https://www.googletagmanager.com/gtm.js?id="+r+s,i.parentNode.insertBefore(a,i)}var timer;function waitForOnetrustActiveGroups(){document.cookie.indexOf("OptanonConsent")>-1&&document.cookie.indexOf("groups=")>-1?(clearTimeout(timer),setGTM(window,document,"script","dataLayer","GTM-TT84L8K")):timer=setTimeout(waitForOnetrustActiveGroups,250)}document.cookie.indexOf("OptanonConsent")>-1&&document.cookie.indexOf("groups=")>-1?setGTM(window,document,"script","dataLayer","GTM-TT84L8K"):waitForOnetrustActiveGroups()</script><link rel="stylesheet" href="/rabbitmq-website/styles.css">
<script src="/rabbitmq-website/runtime~main.js" defer="defer"></script>
<script src="/rabbitmq-website/main.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();null!==e?t(e):window.matchMedia("(prefers-color-scheme: dark)").matches?t("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,t("light"))}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><div class="announcementBar_mb4j" style="background-color:var(--ifm-color-primary-contrast-background);color:var(--ifm-font-color-base)" role="banner"><div class="announcementBarPlaceholder_vyr4"></div><div class="content_knG7 announcementBarContent_xLdY"><strong style="font-size: var(--ifm-h4-font-size);"><a href="https://github.com/rabbitmq/rabbitmq-server/releases/tag/v4.1.0">RabbitMQ 4.1.0 is out</a></strong></div><button type="button" aria-label="Close" class="clean-btn close closeButton_CVFx announcementBarClose_gvF7"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/rabbitmq-website/"><div class="navbar__logo"><img src="/rabbitmq-website/img/rabbitmq-logo-with-name.svg" alt="RabbitMQ" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/rabbitmq-website/img/rabbitmq-logo-with-name.svg" alt="RabbitMQ" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><a class="navbar__item navbar__link" href="/rabbitmq-website/tutorials">Getting Started</a><a class="navbar__item navbar__link" href="/rabbitmq-website/docs">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/rabbitmq-website/blog">Blog</a><a class="navbar__item navbar__link" href="/rabbitmq-website/contact">Support</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a class="navbar__link" aria-haspopup="true" aria-expanded="false" role="button" href="/rabbitmq-website/docs">4.1</a><ul class="dropdown__menu"><li class=""><strong>Release series</strong></li><li><a class="dropdown__link" href="/rabbitmq-website/docs/next">Next</a></li><li><a class="dropdown__link" href="/rabbitmq-website/docs">4.1</a></li><li><a class="dropdown__link" href="/rabbitmq-website/docs/4.0">4.0</a></li><li><a class="dropdown__link" href="/rabbitmq-website/docs/3.13">3.13</a></li><li><a href="https://v3-12.rabbitmq.com/documentation.html" target="_blank" rel="noopener noreferrer" class="dropdown__link">3.12<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a class="dropdown__link" href="/rabbitmq-website/release-information">Release Information</a></li></ul></div><a href="https://github.com/rabbitmq/rabbitmq-website" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><main class="col col--9 col--offset-1"><article class=""><header><h1 class="title_f1Hy">Quorum Queues and Flow Control - Single Queue Benchmarks</h1><div class="container_mt6G margin-vert--md"><time datetime="2020-05-14T00:00:00.000Z">May 14, 2020</time> · <!-- -->13 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><span class="authorName_yefp">Jack Vanlightly</span></div><div class="authorSocials_rSDt"></div></div></div></div></div></header><div id="__blog-post-container" class="markdown"><p>In the last post we covered what flow control is, both as a general concept and the various flow control mechanisms available in RabbitMQ. We saw that publisher confirms and consumer acknowledgements are not just data safety measures, but also play a role in flow control. </p>
<p>In this post we’re going to look at how application developers can use publisher confirms and consumer acknowledgements to get a balance of safety and high performance, in the context of a single queue. </p>
<p>Flow control becomes especially important when a broker is being overloaded. A single queue is unlikely to overload your broker. If you send large messages then sure, you can saturate your network, or if you only have a single CPU core, then one queue could max it out. But most of us are on 8, 16 or 30+ core machines. But it’s interesting to break down the effects of confirms and acks on a single queue. From there we can take our learnings and see if they apply to larger deployments (the next post).</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="pipelining-and-publisher-confirms">Pipelining and Publisher Confirms<a href="#pipelining-and-publisher-confirms" class="hash-link" aria-label="Direct link to Pipelining and Publisher Confirms" title="Direct link to Pipelining and Publisher Confirms">​</a></h2>
<p>Publishers can rate limit themselves based on the number of unconfirmed messages in-flight. With a limit of 5, the publisher will send 5 messages and then block until confirms come in. If a single confirm comes in, the publisher can now send a message and block again. If three confirms come in, the publisher can send three more, and so on.</p>
<p>Confirms can be batched together via the use of the <em>multiple</em> flag. This allows the broker to confirm multiple messages at a time. If 100 messages are pending confirmation, with sequence numbers 1-100, the broker can send a single confirm with the multiple flag set and the sequence number of 100. This allows for less communications between publisher and broker, which is more efficient.</p>
<p>This pipelining method will produce the highest and most stable throughput. You can find code samples for how to do this in tutorial 7, strategy #3. There is a <a href="/rabbitmq-website/tutorials/tutorial-seven-java">Java</a> version and a <a href="/rabbitmq-website/tutorials/tutorial-seven-dotnet">C#</a> version. The same approach can be applied to other languages.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="pipelining-and-consumer-acks">Pipelining and Consumer Acks<a href="#pipelining-and-consumer-acks" class="hash-link" aria-label="Direct link to Pipelining and Consumer Acks" title="Direct link to Pipelining and Consumer Acks">​</a></h2>
<p>RabbitMQ employs the pipelining method, where its “in-flight limit” is the consumer prefetch (QoS) on a given channel. Without a prefetch, it will send messages as fast as it can until there are no more messages to send or TCP back-pressure is applied due to the client TCP buffer being full. This can overload the consumer so please use a prefetch!</p>
<p>When your consumer sends an acknowledgement, that is like when the broker sends a confirm to a publisher. It allows more messages to be pushed down the channel.</p>
<p>Just like confirms, consumer acks get to use the multiple flag as well. The consumer can choose to acknowledge every message individually or every N number of messages. We’ll call this the <em>ack interval</em>. With an ack interval of 10, a consumer will acknowledge every 10th message, using the multiple flag. </p>
<p>This can be more complex code as you also need to take into account:</p>
<ul>
<li>If the last 10 messages includes a mix of acks, nacks and rejects then you can’t simply perform a single ack with multiple flag set.</li>
<li>You will probably wish to put a temporal limit on the length of time between acks, in case messages come in slowly, for example, every 10 messages or up to 50ms.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="measuring-the-impact-of-in-flight-limit-prefetch-and-ack-interval">Measuring the Impact of In-Flight Limit, Prefetch and Ack Interval<a href="#measuring-the-impact-of-in-flight-limit-prefetch-and-ack-interval" class="hash-link" aria-label="Direct link to Measuring the Impact of In-Flight Limit, Prefetch and Ack Interval" title="Direct link to Measuring the Impact of In-Flight Limit, Prefetch and Ack Interval">​</a></h2>
<p>The best way to see the impact is to run a series of benchmarks with a typical cluster and change the publisher confirm in-flight limit, the consumer prefetch and the ack interval.</p>
<p>All benchmarks are run in AWS with the following configuration:</p>
<ul>
<li>c5.4xlarge EC2 instance: 16 vCPUs (Cascade Lake/Skylake Xeon), 32 GB RAM, 5gbit network, 200 GB SSD (io1 with 10000 IOPS)</li>
<li>3 brokers in a cluster</li>
<li>1 load generation EC2 machine of the same spec (c5.4xlarge)</li>
<li>1kb messages</li>
<li>No processing time as this is a pure throughput/latency benchmark of a single publisher and single consumer.</li>
</ul>
<p>We test quorum queues and mirrored queues to give an idea of how quorum queues differ from their older counterpart.</p>
<p>Mirrored queues have one master and one mirror, and quorum queues use a replication factor of three (one leader + two followers). It’s not exactly a fair fight, a replication factor of two with mirrored and a replication factor of three with quorum queues, but those are the most common configurations respectively. All tests use an alpha build of RabbitMQ 3.8.4 with new quorum queue features for handling high load. </p>
<p>Benchmarks:</p>
<ol>
<li>Increasing in-flight limit, prefetch 1000, ack interval 1</li>
<li>1000 in-flight limit, increasing prefetch, ack interval 1</li>
<li>1000 in-flight limit, 1000 prefetch, increasing ack interval</li>
<li>No confirms, no acks</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="interpreting-these-results">Interpreting these results<a href="#interpreting-these-results" class="hash-link" aria-label="Direct link to Interpreting these results" title="Direct link to Interpreting these results">​</a></h2>
<p>The rules:</p>
<ul>
<li><strong>Rule 1</strong> - These are synthetic benchmarks, with a specific version of RabbitMQ, with cloud instances (which introduces all kinds of reproducibility issues) and a specific hardware configuration. There is no single benchmark result, there are infinite. So don’t look at specific numbers, look at trends and patterns.</li>
<li><strong>Rule 2</strong> - These results are using the Java client, not Spring, not Python or any other language or framework. However, what we are testing should hold true for other frameworks as they have to use the same settings, how they use those settings may or may not be under your control.</li>
<li><strong>Rule 3</strong> - Try out your existing code with these different settings changes and see for yourself!</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="benchmark-1---increasing-in-flight-limit-prefetch-1000-ack-interval-1">Benchmark #1 - Increasing in-flight limit, prefetch 1000, ack interval 1<a href="#benchmark-1---increasing-in-flight-limit-prefetch-1000-ack-interval-1" class="hash-link" aria-label="Direct link to Benchmark #1 - Increasing in-flight limit, prefetch 1000, ack interval 1" title="Direct link to Benchmark #1 - Increasing in-flight limit, prefetch 1000, ack interval 1">​</a></h2>
<p>This is a 30 minute benchmark where we increase the in-flight limit every 5 minutes with the following values: 1, 5, 20, 200, 1000, 10000. With the low values, the publishers will be rate limiting themselves pretty aggressively, constraining throughput, but as the limit increases we should see throughput increase.</p>
<p><strong>Mirrored queue</strong></p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 1. Mirrored queue with increasing publisher confirm in-flight limit" src="/rabbitmq-website/assets/images/1-pub-queue-con-mirrored-increasing-inflight-1453ed646cddf5254cdbad968c766245.png" width="910" height="432" class="img_ev3q"><figcaption>Fig 1. Mirrored queue with increasing publisher confirm in-flight limit</figcaption></figure><p></p>
<p><strong>Quorum queue</strong></p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 2. Quorum queue with increasing publisher confirm in-flight limit" src="/rabbitmq-website/assets/images/1-pub-queue-con-qq-increasing-inflight-9047d48c585ecf19fc454c197d1c7680.png" width="912" height="433" class="img_ev3q"><figcaption>Fig 2. Quorum queue with increasing publisher confirm in-flight limit</figcaption></figure><p></p>
<p>Both queue types have a similar profile. As we increase the in-flight limit, throughput goes up until we see that the level is so high as to not have any kind of flow control effect. Both see the biggest jump between 20 and 200. A limit of 10000 has no benefit over 1000, all that happens is we increase end-to-end latency.</p>
<p>The quorum queue achieves much higher throughput than the mirrored queue and also has lower 95th percentile latencies. Quorum queue 99.9th percentile latencies reach the mirrored queue latencies where all percentiles cluster around the same value.</p>
<p>In our case, because the brokers and the load generator are all in the same availability zone, network latency is very low. In higher network latency scenarios, we would continue to see large benefits of higher in-flight limits.</p>
<p>Lastly, remember that if our message rate were 1000 msg/s, then all in-flight limits would look the same. So if you are nowhere close to the queue throughput limit, then these settings won&#x27;t necessarily come into play.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="benchmark-2---1000-in-flight-limit-increasing-prefetch-ack-interval-1">Benchmark #2 - 1000 in-flight limit, increasing prefetch, ack interval 1<a href="#benchmark-2---1000-in-flight-limit-increasing-prefetch-ack-interval-1" class="hash-link" aria-label="Direct link to Benchmark #2 - 1000 in-flight limit, increasing prefetch, ack interval 1" title="Direct link to Benchmark #2 - 1000 in-flight limit, increasing prefetch, ack interval 1">​</a></h2>
<p>This test is a little different to the others. The others are a single run where we dynamically change the behaviour of the load generator. In this test we use a separate run per setting. We have to do this because you’ll see that a prefetch of 1 makes the consume rate so slow, that the queue fills up fast and negatively affects the later phases of the test. So we run each prefetch setting as a completely isolated run.</p>
<p><strong>Mirrored queue</strong></p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 3. Mirrored queue with increasing consumer prefetch." src="/rabbitmq-website/assets/images/1-pub-queue-con-mirrored-increasing-prefetch-ff5980713f6d66d6af9dee304e26ba2b.png" width="910" height="433" class="img_ev3q"><figcaption>Fig 3. Mirrored queue with increasing consumer prefetch.</figcaption></figure><p></p>
<p><strong>Quorum queue</strong></p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 4. Quorum queue with increasing consumer prefetch." src="/rabbitmq-website/assets/images/1-pub-queue-con-qq-increasing-prefetch-e5ed740ca2da205b5bc9102484dff042.png" width="912" height="432" class="img_ev3q"><figcaption>Fig 4. Quorum queue with increasing consumer prefetch.</figcaption></figure><p></p>
<p>A prefetch of 1, combined with a fast publisher did not go well for either queue type, but quorum queues did especially badly. Quorum queues saw very low consumer throughput with a prefetch 1 and 10, but we also saw the publish rate drop as time went by and the queue filled. </p>
<p>In fact in these first two tests (prefetch 1 and 10), the quorum queue reached around 4 million messages. We know that quorum queues do slow down a bit once they get into the millions of messages.</p>
<p>From a prefetch of 100 and onwards we start hitting the top throughput as the RabbitMQ consumer channel is not having to block so often (waiting for acks to come in). Setting a high prefetch does not affect end-to-end latency as we see below (for prefetch of 100, 1000, 10000). </p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 5. End-to-end latency for both queue types with 100, 1000 and 10000 prefetch." src="/rabbitmq-website/assets/images/1-pub-queue-con-qq-mirrored-increasing-prefetch-lat-79465dacf3e428078e05bc8b62f4f2aa.png" width="897" height="437" class="img_ev3q"><figcaption>Fig 5. End-to-end latency for both queue types with 100, 1000 and 10000 prefetch.</figcaption></figure><p></p>
<p>The reason that prefetch doesn’t necessarily increase latency but the in-flight limit can, is that with the in-flight limit we are rate limiting ingress, avoiding buffering in the broker, whereas the prefetch only affects messages already in flight. Whether the messages are buffered in the broker or in the client doesn’t affect latency, especially in a single consumer test. In a multiple consumer test it is conceivable that there could still be an effect.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="some-nuance-around-end-to-end-latency">Some Nuance Around End-to-end Latency<a href="#some-nuance-around-end-to-end-latency" class="hash-link" aria-label="Direct link to Some Nuance Around End-to-end Latency" title="Direct link to Some Nuance Around End-to-end Latency">​</a></h3>
<p>Of course the above is predicated on end-to-end latency being from the moment a publisher sends a message to the moment the message is received by a consumer. In your system, end-to-end latency will likely start at an earlier point. So rate limiting the publisher can reduce latency from the point of view of RabbitMQ, but not necessarily your wider system. When it would definitely affect your wider system&#x27;s end-to-end latency is if RabbitMQ got overloaded and materially slowed down.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="benchmark-3---1000-in-flight-limit-1000-prefetch-increasing-ack-interval">Benchmark #3 - 1000 in-flight limit, 1000 prefetch, increasing ack interval<a href="#benchmark-3---1000-in-flight-limit-1000-prefetch-increasing-ack-interval" class="hash-link" aria-label="Direct link to Benchmark #3 - 1000 in-flight limit, 1000 prefetch, increasing ack interval" title="Direct link to Benchmark #3 - 1000 in-flight limit, 1000 prefetch, increasing ack interval">​</a></h2>
<p>We’re back to the dynamic update of settings again, as we’ll see that while the ack interval does affect throughput, it does not affect it as much as prefetch (not even close!). Using an ack interval of 1 is ok, you will still get good throughput, so if that is what you already do and don’t want the complexity of multiple flag usage, then carry on.</p>
<p>But we’ll see next that if you want every last bit of performance, multiple flag usage helps.</p>
<p><strong>Mirrored queue</strong></p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 6. Mirrored queue with increasing ack interval" src="/rabbitmq-website/assets/images/1-pub-queue-con-mirrored-increasing-ack-interval-204b9574c596e917f85484ad35e463ed.png" width="906" height="430" class="img_ev3q"><figcaption>Fig 6. Mirrored queue with increasing ack interval</figcaption></figure><p></p>
<p><strong>Quorum queue</strong></p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 7. Quorum queue with increasing ack interval" src="/rabbitmq-website/assets/images/1-pub-queue-con-qq-increasing-ack-interval-39ca3967f65d8bdad616687b97bcf200.png" width="909" height="429" class="img_ev3q"><figcaption>Fig 7. Quorum queue with increasing ack interval</figcaption></figure><p></p>
<p>Both queue types see the biggest jump in throughput when switching from an ack interval of 1 to 10. After that the peak is around 50-100. That is 5% and 10% respectively of the prefetch. As a general rule of thumb, this tends to be the sweet spot for the ack interval. </p>
<p>Mirrored queues tend to see a reduction in throughput once you get past the 25-30% of prefetch mark and rapidly drops off past 50%. Quorum queues remained flat in this test, right up to 50%. </p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="benchmarks-4---no-confirms-and-acks">Benchmarks #4 - No confirms and acks<a href="#benchmarks-4---no-confirms-and-acks" class="hash-link" aria-label="Direct link to Benchmarks #4 - No confirms and acks" title="Direct link to Benchmarks #4 - No confirms and acks">​</a></h2>
<p>In these tests we&#x27;ll not use publisher confirms and the consumer will use auto ack mode (this means that the broker will treat a message as delivered as soon as it transmits it).</p>
<p><strong>Mirrored queue</strong></p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 8. Mirrored queue without publisher confirms or consumer acks" src="/rabbitmq-website/assets/images/1-pub-mirrored-queue-con-no-confirms-acks-578f2cedd95a27401fdb556e8e47f6e3.png" width="907" height="428" class="img_ev3q"><figcaption>Fig 8. Mirrored queue without publisher confirms or consumer acks</figcaption></figure><p></p>
<p><strong>Quorum queue</strong></p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 9. Quorum queue without publisher confirms or consumer acks" src="/rabbitmq-website/assets/images/1-pub-qq-con-no-confirms-acks-05fc708b0ddf32cb6a16e3acad3b5205.png" width="912" height="434" class="img_ev3q"><figcaption>Fig 9. Quorum queue without publisher confirms or consumer acks</figcaption></figure><p></p>
<p>If we compare those results to using confirms and acks, we see no benefit in throughput. In fact all we see is an increase in end-to-end latency. For mirrored we go from 95th percentile at ~60ms to ~1s. Likewise for quorum queues we go from 95th percentile ~50ms to ~400ms.</p>
<p>So not only do we not see an increase in throughput but we see a worse latency. Of course this is a single queue, things only get worse as we add more queues and load, as we’ll see in the next post. </p>
<p>With a non-replicated classic queue, you will definitely see a difference between confirms/acks vs none. This is because without replication, RabbitMQ doesn’t have to do much work, so the overhead of confirms and acks is noticeable. This isn’t the case when replication is involved, the overhead of confirms/acks is small in comparison.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="final-conclusions">Final Conclusions<a href="#final-conclusions" class="hash-link" aria-label="Direct link to Final Conclusions" title="Direct link to Final Conclusions">​</a></h2>
<p>At this point, with a single queue, the conclusions are simple and limited - that is, they apply to a single queue for sure, and will likely apply to multiple queues but not necessarily a stressed system. That is why we have a follow-up post covering exactly the same settings, but with a system under stress.</p>
<p>Low broker stress, single queue, high throughput conclusions:</p>
<ol>
<li>Low publisher in-flight limits equate to lower throughput as publishers exert their own flow control. Higher in-flight limits equate to higher throughput, but at some point you stop getting gains. Where that point is, is totally dependent on your system and can change as conditions in your system change.</li>
<li>Low prefetch can be terrible for a high throughput queue with a single consumer. But in a low throughput queue or where there are many many consumers, it will not be so much of a problem (as we’ll see in the next post where we have 100s of consumers).</li>
<li>An ack interval of 1 is ok, don’t sweat it. But increasing it a little can be beneficial. Up to around 10% of prefetch is a good rule of thumb but as always, it is dependent on your system and local conditions.</li>
<li>Confirms and acks are necessary for data safety and not using them with a replicated queue doesn’t gain you any performance, quite the opposite, it increased latency. That said, in this single queue test, the loss of the extra flow control exerted by confirms and acks was not a major problem.</li>
<li>Finally - a single quorum queue outperforms a single mirrored queue significantly.</li>
</ol>
<p>All these tests were about sending/consuming messages as fast as possible, pushing a single queue to its limit. What we learned is informative but you likely are not in that situation and so you will probably find the next post more useful. In that post we will look at low load and high load scenarios, with different numbers of queues and clients, but seeing the effects of these same three settings on both quorum and mirrored queues. For the stress tests, flow control will become more important, it will help the stressed system to degrade gracefully rather than catch on fire. Expect a larger impact of not using confirms and acks.</p></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/rabbitmq-website/blog/tags/performance">Performance</a></li></ul></div></div><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><a href="https://github.com/rabbitmq/rabbitmq-website/tree/main/blog/2020-05-14-quorum-queues-and-flow-control-single-queue-benchmarks/index.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/rabbitmq-website/blog/2020/05/15/quorum-queues-and-flow-control-stress-tests"><div class="pagination-nav__sublabel">Newer post</div><div class="pagination-nav__label">Quorum Queues and Flow Control - Stress Tests</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/rabbitmq-website/blog/2020/05/04/quorum-queues-and-flow-control-the-concepts"><div class="pagination-nav__sublabel">Older post</div><div class="pagination-nav__label">Quorum Queues and Flow Control - The Concepts</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#pipelining-and-publisher-confirms" class="table-of-contents__link toc-highlight">Pipelining and Publisher Confirms</a></li><li><a href="#pipelining-and-consumer-acks" class="table-of-contents__link toc-highlight">Pipelining and Consumer Acks</a></li><li><a href="#measuring-the-impact-of-in-flight-limit-prefetch-and-ack-interval" class="table-of-contents__link toc-highlight">Measuring the Impact of In-Flight Limit, Prefetch and Ack Interval</a></li><li><a href="#interpreting-these-results" class="table-of-contents__link toc-highlight">Interpreting these results</a></li><li><a href="#benchmark-1---increasing-in-flight-limit-prefetch-1000-ack-interval-1" class="table-of-contents__link toc-highlight">Benchmark #1 - Increasing in-flight limit, prefetch 1000, ack interval 1</a></li><li><a href="#benchmark-2---1000-in-flight-limit-increasing-prefetch-ack-interval-1" class="table-of-contents__link toc-highlight">Benchmark #2 - 1000 in-flight limit, increasing prefetch, ack interval 1</a><ul><li><a href="#some-nuance-around-end-to-end-latency" class="table-of-contents__link toc-highlight">Some Nuance Around End-to-end Latency</a></li></ul></li><li><a href="#benchmark-3---1000-in-flight-limit-1000-prefetch-increasing-ack-interval" class="table-of-contents__link toc-highlight">Benchmark #3 - 1000 in-flight limit, 1000 prefetch, increasing ack interval</a></li><li><a href="#benchmarks-4---no-confirms-and-acks" class="table-of-contents__link toc-highlight">Benchmarks #4 - No confirms and acks</a></li><li><a href="#final-conclusions" class="table-of-contents__link toc-highlight">Final Conclusions</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Learn about RabbitMQ</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/rabbitmq-website/tutorials">Getting Started</a></li><li class="footer__item"><a class="footer__link-item" href="/rabbitmq-website/docs">Documentation</a></li><li class="footer__item"><a class="footer__link-item" href="/rabbitmq-website/blog">Blog</a></li></ul></div><div class="col footer__col"><div class="footer__title">Reach out to the RabbitMQ team</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/rabbitmq" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/rabbitmq/rabbitmq-server/discussions" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub Discussions<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a class="footer__link-item" href="/rabbitmq-website/contact?utm_source=rmq_release-information_tableheader&amp;utm_medium=rmq_website&amp;utm_campaign=tanzu">Long Term Commercial Support</a></li><li class="footer__item"><a class="footer__link-item" href="/rabbitmq-website/contact">Contact Us</a></li><li class="footer__item"><a href="https://www.rabbitmq.com/discord" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Broadcom</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://tanzu.vmware.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">VMware Tanzu<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.vmware.com/help/legal.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">Terms of Use<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.vmware.com/help/privacy.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">Privacy<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a class="footer__link-item" href="/rabbitmq-website/trademark-guidelines">Trademark Guidelines</a></li><li class="footer__item"><a href="https://www.vmware.com/help/privacy/california-privacy-rights.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">Your California Privacy Rights<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a class="footer__link-item ot-sdk-show-settings">Cookie Settings</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2005-2025 Broadcom. All Rights Reserved. The term "Broadcom" refers to Broadcom Inc. and/or its subsidiaries.</div></div></div></footer></div>
</body>
</html>