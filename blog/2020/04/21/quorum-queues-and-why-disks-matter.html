<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">Quorum queues and why disks matter | RabbitMQ</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://www.rabbitmq.com/rabbitmq-website/img/rabbitmq-social-media-card.svg"><meta data-rh="true" name="twitter:image" content="https://www.rabbitmq.com/rabbitmq-website/img/rabbitmq-social-media-card.svg"><meta data-rh="true" property="og:url" content="https://www.rabbitmq.com/rabbitmq-website/blog/2020/04/21/quorum-queues-and-why-disks-matter"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Quorum queues and why disks matter | RabbitMQ"><meta data-rh="true" name="description" content="Quorum queues are still relatively new to RabbitMQ and many people have still not made the jump from classic mirrored queues. Before you migrate to this new queue type you need to make sure that your hardware can support your workload and a big factor in that is what storage drives you use."><meta data-rh="true" property="og:description" content="Quorum queues are still relatively new to RabbitMQ and many people have still not made the jump from classic mirrored queues. Before you migrate to this new queue type you need to make sure that your hardware can support your workload and a big factor in that is what storage drives you use."><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2020-04-21T00:00:00.000Z"><meta data-rh="true" property="article:tag" content="Performance"><link data-rh="true" rel="icon" href="/rabbitmq-website/img/rabbitmq-logo.svg"><link data-rh="true" rel="canonical" href="https://www.rabbitmq.com/rabbitmq-website/blog/2020/04/21/quorum-queues-and-why-disks-matter"><link data-rh="true" rel="alternate" href="https://www.rabbitmq.com/rabbitmq-website/blog/2020/04/21/quorum-queues-and-why-disks-matter" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.rabbitmq.com/rabbitmq-website/blog/2020/04/21/quorum-queues-and-why-disks-matter" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://H10VQIW16Y-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://www.rabbitmq.com/rabbitmq-website/blog/2020/04/21/quorum-queues-and-why-disks-matter","mainEntityOfPage":"https://www.rabbitmq.com/rabbitmq-website/blog/2020/04/21/quorum-queues-and-why-disks-matter","url":"https://www.rabbitmq.com/rabbitmq-website/blog/2020/04/21/quorum-queues-and-why-disks-matter","headline":"Quorum queues and why disks matter","name":"Quorum queues and why disks matter","description":"Quorum queues are still relatively new to RabbitMQ and many people have still not made the jump from classic mirrored queues. Before you migrate to this new queue type you need to make sure that your hardware can support your workload and a big factor in that is what storage drives you use.","datePublished":"2020-04-21T00:00:00.000Z","author":{"@type":"Person","name":"Jack Vanlightly"},"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://www.rabbitmq.com/rabbitmq-website/blog","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/rabbitmq-website/blog/rss.xml" title="RabbitMQ RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/rabbitmq-website/blog/atom.xml" title="RabbitMQ Atom Feed">




<link rel="search" type="application/opensearchdescription+xml" title="RabbitMQ" href="/rabbitmq-website/opensearch.xml">







<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway:400,700">
<script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-domain-script="018ee308-473e-754f-b0c2-cbe82d25512f"></script>
<script>function OptanonWrapper(){}</script>
<script>function setGTM(e,t,o,n,r){e[n]=e[n]||[],e[n].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var i=t.getElementsByTagName(o)[0],a=t.createElement(o),s="dataLayer"!=n?"&l="+n:"";a.async=!0,a.src="https://www.googletagmanager.com/gtm.js?id="+r+s,i.parentNode.insertBefore(a,i)}var timer;function waitForOnetrustActiveGroups(){document.cookie.indexOf("OptanonConsent")>-1&&document.cookie.indexOf("groups=")>-1?(clearTimeout(timer),setGTM(window,document,"script","dataLayer","GTM-TT84L8K")):timer=setTimeout(waitForOnetrustActiveGroups,250)}document.cookie.indexOf("OptanonConsent")>-1&&document.cookie.indexOf("groups=")>-1?setGTM(window,document,"script","dataLayer","GTM-TT84L8K"):waitForOnetrustActiveGroups()</script><link rel="stylesheet" href="/rabbitmq-website/styles.css">
<script src="/rabbitmq-website/runtime~main.js" defer="defer"></script>
<script src="/rabbitmq-website/main.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();null!==e?t(e):window.matchMedia("(prefers-color-scheme: dark)").matches?t("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,t("light"))}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><div class="announcementBar_mb4j" style="background-color:var(--ifm-color-primary-contrast-background);color:var(--ifm-font-color-base)" role="banner"><div class="announcementBarPlaceholder_vyr4"></div><div class="content_knG7 announcementBarContent_xLdY"><strong style="font-size: var(--ifm-h4-font-size);"><a href="https://github.com/rabbitmq/rabbitmq-server/releases/tag/v4.1.0">RabbitMQ 4.1.0 is out</a></strong></div><button type="button" aria-label="Close" class="clean-btn close closeButton_CVFx announcementBarClose_gvF7"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/rabbitmq-website/"><div class="navbar__logo"><img src="/rabbitmq-website/img/rabbitmq-logo-with-name.svg" alt="RabbitMQ" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/rabbitmq-website/img/rabbitmq-logo-with-name.svg" alt="RabbitMQ" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><a class="navbar__item navbar__link" href="/rabbitmq-website/tutorials">Getting Started</a><a class="navbar__item navbar__link" href="/rabbitmq-website/docs">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/rabbitmq-website/blog">Blog</a><a class="navbar__item navbar__link" href="/rabbitmq-website/contact">Support</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a class="navbar__link" aria-haspopup="true" aria-expanded="false" role="button" href="/rabbitmq-website/docs">4.1</a><ul class="dropdown__menu"><li class=""><strong>Release series</strong></li><li><a class="dropdown__link" href="/rabbitmq-website/docs/next">Next</a></li><li><a class="dropdown__link" href="/rabbitmq-website/docs">4.1</a></li><li><a class="dropdown__link" href="/rabbitmq-website/docs/4.0">4.0</a></li><li><a class="dropdown__link" href="/rabbitmq-website/docs/3.13">3.13</a></li><li><a href="https://v3-12.rabbitmq.com/documentation.html" target="_blank" rel="noopener noreferrer" class="dropdown__link">3.12<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a class="dropdown__link" href="/rabbitmq-website/release-information">Release Information</a></li></ul></div><a href="https://github.com/rabbitmq/rabbitmq-website" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><main class="col col--9 col--offset-1"><article class=""><header><h1 class="title_f1Hy">Quorum queues and why disks matter</h1><div class="container_mt6G margin-vert--md"><time datetime="2020-04-21T00:00:00.000Z">April 21, 2020</time> · <!-- -->13 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><span class="authorName_yefp">Jack Vanlightly</span></div><div class="authorSocials_rSDt"></div></div></div></div></div></header><div id="__blog-post-container" class="markdown"><p>Quorum queues are still relatively new to RabbitMQ and many people have still not made the jump from classic mirrored queues. Before you migrate to this new queue type you need to make sure that your hardware can support your workload and a big factor in that is what storage drives you use.</p>
<p>In this blog post we’re going to take a closer look at quorum queues and their performance characteristics on different storage configurations.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="hdd-or-ssd-one-drive-or-multiple-drives">HDD or SSD? One drive or multiple drives?<a href="#hdd-or-ssd-one-drive-or-multiple-drives" class="hash-link" aria-label="Direct link to HDD or SSD? One drive or multiple drives?" title="Direct link to HDD or SSD? One drive or multiple drives?">​</a></h2>
<p>The TL;DR is that we highly recommend SSDs when using quorum queues. The reason for this is that quorum queues are sensitive to IO latency and SSDs deliver lower latency IO than HDDs. With higher IO latency, you&#x27;ll see lower throughput, higher end-to-end latency and some other undesirable effects.</p>
<p>Further down in this post we’ll demonstrate why we recommend this, using various benchmarks with different SSD and HDD configurations.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-are-quorum-queues-sensitive-to-io-latency">Why are quorum queues sensitive to IO latency?<a href="#why-are-quorum-queues-sensitive-to-io-latency" class="hash-link" aria-label="Direct link to Why are quorum queues sensitive to IO latency?" title="Direct link to Why are quorum queues sensitive to IO latency?">​</a></h2>
<p>Let&#x27;s look at the write path on a single broker to see why this is the case. Let&#x27;s remember that both publishing and consuming count as writes. Publishing involves <em>enqueue</em> operations and consuming involves <em>ack</em> operations, and both must be persisted and replicated. Note that if you use a quorum or mirrored queues without publisher confirms or consumer acknowledgements you need to ask yourself why you are using a replicated queue.</p>
<p>Operations first get written to memory and to a Write Ahead Log (WAL). There is a single WAL per broker, which serves all quorum queues on that broker. From there, operations are then written to per-queue segment files by the Segment Writer.</p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 1. WAL and segment files" src="/rabbitmq-website/assets/images/wal-and-segments-6ba0eda69e28558abce38f8c5f09bb1d.png" width="923" height="490" class="img_ev3q"><figcaption>Fig 1. WAL and segment files</figcaption></figure><p></p>
<blockquote>
<p>There is an optimisation however that means enqueue operations (messages) may never need to be written to segment files. Newly arrived messages are kept in memory and messages can be delivered to and acked by consumers before having been written to a segment file. In this case, those messages are not written to disk as they essentially no longer exist as far as the broker is concerned.</p>
</blockquote>
<p>This means that for queues where consumers are keeping up, often messages do not get written to segment files at all. By the time an in memory message is ready to be flushed to a segment file, a consumer has acked the message.
If IO latency is high, then the WAL ends up being a bottleneck. Each broker has a single WAL and a single segment writer process, and these act as shared infrastructure that the quorum queue Raft clusters sit on top of. The fsyncing of operations to the active WAL  file is like the beating heart of the Raft clusters. If fsyncs are slow, the throughput across the Raft clusters is slow.</p>
<p>Fsyncing of the segment files can also become a bottleneck. The WAL consists of one or more files; one file being actively written to and zero or more inactive files that were rolled due to reaching the max WAL file size limit. These inactive WAL files can only be safely removed once all their messages have been written to segment files and/or acknowledged (see optimisation above).  If the writing of segment files is slow, then the WAL gets larger and larger as messages cannot be written to segment files fast enough.</p>
<p>HDDs are great at large sequential writes but not so great at small or random IO. If your writes have to access multiple files, jumping about the filesystem, then HDDs produce much higher IO latency than SSDs. The great thing about WAL and segment files is that they are append-only files. So HDDs have the potential to deliver decent performance, but only where there is not a lot of random IO contention. As soon as WAL and segment file IO has to contend with other disk IO, performance starts to drop.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="ok-but-what-does-performance-with-hdds-look-like">Ok, but what does performance with HDDs look like?<a href="#ok-but-what-does-performance-with-hdds-look-like" class="hash-link" aria-label="Direct link to Ok, but what does performance with HDDs look like?" title="Direct link to Ok, but what does performance with HDDs look like?">​</a></h2>
<p>Now we’re going to run some performance benchmarks with both SSDs and HDDs and analyse the results.</p>
<p>There are four main actors that use disk:</p>
<ul>
<li>Mnesia and the message store (data)</li>
<li>Logs (logs)</li>
<li>Quorum queue segment files (segment)</li>
<li>Quorum queue WAL (wal)</li>
</ul>
<p>Each of these workloads has different IO patterns and we can try to isolate these disk workloads to improve performance.</p>
<p>We have six clusters that share certain aspects such as number of CPUs but use different types and number of disks.</p>
<p>Shared:</p>
<ul>
<li>3 node clusters</li>
<li>Instance type: c4.4xlarge:<!-- -->
<ul>
<li>16 vCPUs</li>
<li>15GB RAM</li>
<li>2Gbps EBS instance throughput (the VM has a disk IO throughput ceiling of 2Gbps/250MBs)</li>
<li>5Gbps network (625MB/s)</li>
</ul>
</li>
</ul>
<p>All disks are either 200GB SSDs (io1) or 1TB HDDs (st1). Each has a larger throughput capacity than the c4.4xlarge EC2 instance can use.</p>
<p>Unique:</p>
<ul>
<li>Store all data in a single drive<!-- -->
<ul>
<li>Cluster rabbitmq1, SSD1=data/logs/segment/wal</li>
<li>Cluster rabbitmq10, HDD1=data/logs/segment/wal</li>
</ul>
</li>
<li>Store the WAL on a separate drive, but segment files on same drive as classic queue data<!-- -->
<ul>
<li>Cluster rabbitmq4, SSD1=data/logs/segment SSD2=wal</li>
<li>Cluster rabbitmq13, HDD1=data/logs/segment HDD2=wal</li>
</ul>
</li>
<li>Assign classic queue data, segment files and WAL files their own dedicated drive each<!-- -->
<ul>
<li>Cluster rabbitmq7, SSD1=data/logs SSD2=segment SSD3=wal</li>
<li>Cluster rabbitmq16, HDD1=data/logs HDD2=segment HDD3=wal</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="pure-quorum-queue-workload">Pure Quorum Queue Workload<a href="#pure-quorum-queue-workload" class="hash-link" aria-label="Direct link to Pure Quorum Queue Workload" title="Direct link to Pure Quorum Queue Workload">​</a></h2>
<p>First we&#x27;ll test workloads that only consist of quorum queues.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="throughput-benchmark-1---one-quorum-queue">Throughput Benchmark #1 - One Quorum Queue<a href="#throughput-benchmark-1---one-quorum-queue" class="hash-link" aria-label="Direct link to Throughput Benchmark #1 - One Quorum Queue" title="Direct link to Throughput Benchmark #1 - One Quorum Queue">​</a></h3>
<p>One publisher, one queue, one consumer, 1kb messages, no rate limit.</p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 1. Pure quorum queue workload - 1 queue" src="/rabbitmq-website/assets/images/pure-qq-b1-810bada1dd00b60e7059465ed6bc0665.png" width="910" height="710" class="img_ev3q"><figcaption>Fig 1. Pure quorum queue workload - 1 queue</figcaption></figure><p></p>
<p>We see that the SSD clusters rabbitmq1, rabbitmq4 and rabbitmq7 all are reaching around 19k msg/s. The HDD clusters have lower throughput, with rabbitmq13 (2 disks) and rabbitmq16 (3 disks) are only slightly behind at around 17k msg/s. The single HDD cluster clearly lags at around 13k msg/s.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h4>
<p>For a single queue workload, separating the WAL from the segment file workload onto separate disks has given us close to SSD performance. </p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="throughput-benchmark-2---four-quorum-queues">Throughput Benchmark #2 - Four Quorum Queues<a href="#throughput-benchmark-2---four-quorum-queues" class="hash-link" aria-label="Direct link to Throughput Benchmark #2 - Four Quorum Queues" title="Direct link to Throughput Benchmark #2 - Four Quorum Queues">​</a></h3>
<p>4 publishers, 4 queues, 4 consumers, 1kb messages, no rate limit.</p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 2. Pure quorum queue workload - 4 queues" src="/rabbitmq-website/assets/images/pure-qq-b2-e2eec748bec376bff6285a2edd1b0626.png" width="906" height="711" class="img_ev3q"><figcaption>Fig 2. Pure quorum queue workload - 4 queues</figcaption></figure><p></p>
<p>With 4 quorum queues, we see a different story. HDDs outperform their SSD counterparts by ~2k msg/s. We have to remember that when consumers can keep up, operations are usually only written to the WAL files. The WAL is shared between the four queues so we are pushing more bytes through it. WAL files are written to sequentially and our st1 HDD can manage 500MB/s throughput (though the VM itself is restricted to 250MB/s).</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion-1">Conclusion<a href="#conclusion-1" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h4>
<p>In a world of only sequential writes, HDDs can potentially produce similar or even better results to an SSD. A quorum queue only workload, with consumers keeping up should see the vast majority of writes only going to a single append-only WAL file.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="latency-benchmark-1---40-quorum-queues">Latency Benchmark #1 - 40 Quorum Queues<a href="#latency-benchmark-1---40-quorum-queues" class="hash-link" aria-label="Direct link to Latency Benchmark #1 - 40 Quorum Queues" title="Direct link to Latency Benchmark #1 - 40 Quorum Queues">​</a></h3>
<p>40 publishers, 40 queues, 40 consumers, 1kb messages, 10 msg/s per publisher (400 msg/s total).</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="ssds">SSDs<a href="#ssds" class="hash-link" aria-label="Direct link to SSDs" title="Direct link to SSDs">​</a></h4>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 3. Pure quorum queue workload - Latency Test 1 - SSD" src="/rabbitmq-website/assets/images/pure-qq-b3-ssd-d453224fe43a3a4fa6bc2c8e02659e8d.png" width="909" height="751" class="img_ev3q"><figcaption>Fig 3. Pure quorum queue workload - Latency Test 1 - SSD</figcaption></figure><p></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="hdds">HDDs<a href="#hdds" class="hash-link" aria-label="Direct link to HDDs" title="Direct link to HDDs">​</a></h4>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 4. Pure quorum queue workload - Latency Test 1 - HDD" src="/rabbitmq-website/assets/images/pure-qq-b3-hdd-f9d43ef28fdedfbd155c0f317cf3ec47.png" width="911" height="751" class="img_ev3q"><figcaption>Fig 4. Pure quorum queue workload - Latency Test 1 - HDD</figcaption></figure><p></p>
<p>With a total rate of just 400 1kb messages a second, we see SSDs and HDDs with comparable end-to-end latencies.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="latency-benchmark-2---40-quorum-queues">Latency Benchmark #2 - 40 Quorum Queues<a href="#latency-benchmark-2---40-quorum-queues" class="hash-link" aria-label="Direct link to Latency Benchmark #2 - 40 Quorum Queues" title="Direct link to Latency Benchmark #2 - 40 Quorum Queues">​</a></h3>
<p>40 publishers, 40 queues, 40 consumers, 1kb messages, 50 msg/s per publisher (2000 msg/s total).</p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 5. Pure quorum queue workload - Latency Test 2 - SSD" src="/rabbitmq-website/assets/images/pure-qq-b4-ssd-f2f7fc27be30adf75e13f948bf2e7667.png" width="910" height="753" class="img_ev3q"><figcaption>Fig 5. Pure quorum queue workload - Latency Test 2 - SSD</figcaption></figure><p></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="hdds-1">HDDs<a href="#hdds-1" class="hash-link" aria-label="Direct link to HDDs" title="Direct link to HDDs">​</a></h4>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 6. Pure quorum queue workload - Latency Test 2 - HDD" src="/rabbitmq-website/assets/images/pure-qq-b4-hdd-34e1e0e102974e72481cae481f51dd0d.png" width="909" height="752" class="img_ev3q"><figcaption>Fig 6. Pure quorum queue workload - Latency Test 2 - HDD</figcaption></figure><p></p>
<p>This time we see HDDs with significantly higher latencies:</p>
<ul>
<li>75th percentile ~4ms vs ~15ms</li>
<li>99.9th percentile ~20ms vs ~110ms</li>
</ul>
<p>We also see that the two disk and three disk HDD configurations have lower latency than the single HDD. There is not much difference between the two and three disk configurations as we have no mnesia or message store data competing with quorum segment data.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion-2">Conclusion<a href="#conclusion-2" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h4>
<p>With higher throughput, we see SSDs producing much lower latencies than HDD.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="light-mixed-workload-classic-lazy-and-quorum">Light Mixed Workload (Classic Lazy and Quorum)<a href="#light-mixed-workload-classic-lazy-and-quorum" class="hash-link" aria-label="Direct link to Light Mixed Workload (Classic Lazy and Quorum)" title="Direct link to Light Mixed Workload (Classic Lazy and Quorum)">​</a></h2>
<p>So far we’ve seen quorum queues tested in isolation, without any classic or mirrored queue load. In this test we’ll see how quorum queues behave in a mixed workload of quorum queues and unreplicated lazy queues. Lazy queues are more disk intensive than normal classic queues and should produce more disk contention.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="throughput-benchmark-1---one-quorum-queue-1">Throughput Benchmark #1 - One Quorum Queue<a href="#throughput-benchmark-1---one-quorum-queue-1" class="hash-link" aria-label="Direct link to Throughput Benchmark #1 - One Quorum Queue" title="Direct link to Throughput Benchmark #1 - One Quorum Queue">​</a></h3>
<p>One publisher, one queue, one consumer, 1kb messages, no rate limit.</p>
<p>Lazy queue workload: 40 publishers, 40 queues, 40 consumers, 16b messages, 10 msg/s per publisher (400 msg/s).</p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 7. Light mixed workload - 1 quorum queue" src="/rabbitmq-website/assets/images/mixed-light-qq-b1-45dd38cfe290e402a626f6671f5a82c6.png" width="908" height="713" class="img_ev3q"><figcaption>Fig 7. Light mixed workload - 1 quorum queue</figcaption></figure><p></p>
<p>We see that with this background lazy queue load, the SSD cluster sees no impact in throughput, sitting at 20k msg/s again. However the single HDD cluster has been seriously impacted, down to just 6k msg/s down from 13k msg/s. The write heavy WAL workload now must compete with the message store which will involve much disk seeking.</p>
<p>The two and three disk HDD clusters do better with just a slight drop in throughput, this is because most writes go to the WAL which has a dedicated disk and still achieves sequential writes.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion-3">Conclusion<a href="#conclusion-3" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h4>
<p>A low non-quorum queue message throughput will impact HDDs more, but the impact can be mitigated by separating workloads onto separate disks.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="throughput-benchmark-2--four-quorum-queues">Throughput Benchmark #2 - Four Quorum Queues<a href="#throughput-benchmark-2--four-quorum-queues" class="hash-link" aria-label="Direct link to Throughput Benchmark #2 - Four Quorum Queues" title="Direct link to Throughput Benchmark #2 - Four Quorum Queues">​</a></h3>
<p>4 publishers, 4 queues, 4 consumers, 1kb messages, no rate limit.</p>
<p>Lazy queue workload: 40 publishers, 40 queues, 40 consumers, 16b messages, 10 msg/s per publisher (400 msg/s).</p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 8. Light mixed workload - 4 quorum queues" src="/rabbitmq-website/assets/images/mixed-light-qq-b2-f308a612b093a52d5d4f6f779f612ac8.png" width="908" height="710" class="img_ev3q"><figcaption>Fig 8. Light mixed workload - 4 quorum queues</figcaption></figure><p></p>
<p>Again we see the same drop in throughput for HDDs, with the single disk suffering the most.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="latency-benchmark-1---40-quorum-queues-1">Latency Benchmark #1 - 40 Quorum Queues<a href="#latency-benchmark-1---40-quorum-queues-1" class="hash-link" aria-label="Direct link to Latency Benchmark #1 - 40 Quorum Queues" title="Direct link to Latency Benchmark #1 - 40 Quorum Queues">​</a></h3>
<p>40 publishers, 40 queues, 40 consumers, 1kb messages, 10 msg/s per publisher (400 msg/s total).</p>
<p>Lazy queue workload: 40 publishers, 40 queues, 40 consumers, 16b messages, 10 msg/s per publisher (400 msg/s).</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="ssds-1">SSDs<a href="#ssds-1" class="hash-link" aria-label="Direct link to SSDs" title="Direct link to SSDs">​</a></h4>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 9. Light mixed workload - Latency Test 1 - SSD" src="/rabbitmq-website/assets/images/mixed-light-qq-b3-ssd-37a2e71bde16f82db2e5c9676304a008.png" width="911" height="754" class="img_ev3q"><figcaption>Fig 9. Light mixed workload - Latency Test 1 - SSD</figcaption></figure><p></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="hdds-2">HDDs<a href="#hdds-2" class="hash-link" aria-label="Direct link to HDDs" title="Direct link to HDDs">​</a></h4>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 10. Light mixed workload - Latency Test 1 - HDD" src="/rabbitmq-website/assets/images/mixed-light-qq-b3-hdd-9a9afb20fb37cd67f50c00bbcb17b2e4.png" width="911" height="750" class="img_ev3q"><figcaption>Fig 10. Light mixed workload - Latency Test 1 - HDD</figcaption></figure><p></p>
<p>Last time, with no mixed workload, we saw SSDs and HDDs with comparable end-to-end latencies. This time however, the single HDD cluster has a major increase in end-to-end latency with 75th at 50ms and 99.9th up to 300ms. The two and three disk configurations fared better with comparable 75th percentile latencies but we saw one peak up to 25ms at 99.9th percentile.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion-4">Conclusion<a href="#conclusion-4" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h4>
<p>With a light mixed workload, the single HDD configuration fares badly, but the configurations with a separate disk for the WAL did almost as good as the SSDs.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="latency-benchmark-2---40-quorum-queues-1">Latency Benchmark #2 - 40 Quorum Queues<a href="#latency-benchmark-2---40-quorum-queues-1" class="hash-link" aria-label="Direct link to Latency Benchmark #2 - 40 Quorum Queues" title="Direct link to Latency Benchmark #2 - 40 Quorum Queues">​</a></h3>
<p>40 publishers, 40 queues, 40 consumers, 1kb messages, 50 msg/s per publisher (2000 msg/s total).</p>
<p>Lazy queue workload: 40 publishers, 40 queues, 40 consumers, 16b messages, 10 msg/s per publisher (400 msg/s).</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="ssds-2">SSDs<a href="#ssds-2" class="hash-link" aria-label="Direct link to SSDs" title="Direct link to SSDs">​</a></h4>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 11. Light mixed workload - Latency Test 2 - SSD" src="/rabbitmq-website/assets/images/mixed-light-qq-b4-ssd-6f57bbd26c945a0c9a5b5cc8a20dbaf8.png" width="909" height="751" class="img_ev3q"><figcaption>Fig 11. Light mixed workload - Latency Test 2 - SSD</figcaption></figure><p></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="hdds-3">HDDs<a href="#hdds-3" class="hash-link" aria-label="Direct link to HDDs" title="Direct link to HDDs">​</a></h4>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 12. Light mixed workload - Latency Test 2 - HDD" src="/rabbitmq-website/assets/images/mixed-light-qq-b4-hdd-324420a9549f4b51cf88421850103c27.png" width="909" height="751" class="img_ev3q"><figcaption>Fig 12. Light mixed workload - Latency Test 2 - HDD</figcaption></figure><p></p>
<p>Again we see the single HDD perform the worst, but this time the two and thee disk HDD cluster fare worse, hovering around the 20-60ms mark, compared to the SSDs which hover around 5-15ms.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion-5">Conclusion<a href="#conclusion-5" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h4>
<p>With a light mixed workload, but higher quorum queue load we see HDDs clearly at a disadvantage to SSDs.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="medium-mixed-workload-classic-lazy-and-quorum">Medium Mixed Workload (Classic Lazy and Quorum)<a href="#medium-mixed-workload-classic-lazy-and-quorum" class="hash-link" aria-label="Direct link to Medium Mixed Workload (Classic Lazy and Quorum)" title="Direct link to Medium Mixed Workload (Classic Lazy and Quorum)">​</a></h2>
<p>This time we’ll increase the lazy queue traffic five fold from 400 msg/s to 2000 msg/s and see how our SSD and HDD clusters fare.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="throughput-benchmark-1---one-quorum-queue-2">Throughput Benchmark #1 - One Quorum Queue<a href="#throughput-benchmark-1---one-quorum-queue-2" class="hash-link" aria-label="Direct link to Throughput Benchmark #1 - One Quorum Queue" title="Direct link to Throughput Benchmark #1 - One Quorum Queue">​</a></h3>
<p>One publisher, one queue, one consumer, 1kb messages, no rate limit.</p>
<p>Lazy queue workload: 40 publishers, 40 queues, 40 consumers, 16b messages, 50 msg/s per publisher (2000 msg/s).</p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 13. Medium mixed workload - 1 quorum queue" src="/rabbitmq-website/assets/images/mixed-medium-qq-b1-8b71da76a984dd10cf7b03c63a44d85a.png" width="908" height="712" class="img_ev3q"><figcaption>Fig 13. Medium mixed workload - 1 quorum queue</figcaption></figure><p></p>
<p>This time HDD throughput has been driven very low. We can see the two and thee disk configurations help, but not by much:</p>
<ul>
<li>1 disk: ~300 msg/s</li>
<li>2 disks: ~1700 msg/s</li>
<li>3 disks: ~2300 msg/s</li>
</ul>
<p>But look at the SSDs, they achieve the same throughput as ever.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion-6">Conclusion<a href="#conclusion-6" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h4>
<p>Once the mixed workload reaches a certain point, quorum queue throughput on HDD drops very low, no matter if you isolate the disk workloads. Obviously there is another factor in play here as the three disk configuration which has isolated segment and WAL file load from classic queues is not enough.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="throughput-benchmark-2---four-quorum-queues-1">Throughput Benchmark #2 - Four Quorum Queues<a href="#throughput-benchmark-2---four-quorum-queues-1" class="hash-link" aria-label="Direct link to Throughput Benchmark #2 - Four Quorum Queues" title="Direct link to Throughput Benchmark #2 - Four Quorum Queues">​</a></h3>
<p>4 publishers, 4 queues, 4 consumers, 1kb messages, no rate limit.</p>
<p>Lazy queue workload: 40 publishers, 40 queues, 40 consumers, 16b messages, 50 msg/s per publisher (2000 msg/s).</p>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 14. Medium mixed workload - 4 quorum queues" src="/rabbitmq-website/assets/images/mixed-medium-qq-b2-7742093db71c95fee15ba1d33979830c.png" width="909" height="710" class="img_ev3q"><figcaption>Fig 14. Medium mixed workload - 4 quorum queues</figcaption></figure><p></p>
<p>This time the single HDD barely managed a single message. The two and three disk configurations managed around 2300 msg/s.</p>
<p>This time note that the single SSD cluster saw an impact from the lazy queue traffic. The two and three SSD clusters saw almost no impact, demonstrating that quorum queues can benefit from disk workload isolation even on SSDs.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="latency-benchmark-1---40-quorum-queues-2">Latency Benchmark #1 - 40 Quorum Queues<a href="#latency-benchmark-1---40-quorum-queues-2" class="hash-link" aria-label="Direct link to Latency Benchmark #1 - 40 Quorum Queues" title="Direct link to Latency Benchmark #1 - 40 Quorum Queues">​</a></h3>
<p>40 publishers, 40 queues, 40 consumers, 1kb messages, 10 msg/s per publisher (400 msg/s total).</p>
<p>Lazy queue workload: 40 publishers, 40 queues, 40 consumers, 16b messages, 50 msg/s per publisher (2000 msg/s).</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="ssds-3">SSDs<a href="#ssds-3" class="hash-link" aria-label="Direct link to SSDs" title="Direct link to SSDs">​</a></h4>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 15. Medium mixed workload - Latency Test 1 - SSD" src="/rabbitmq-website/assets/images/mixed-medium-qq-b3-ssd-c9b6d1396e7d0ddc0ed72722ad4e90ce.png" width="911" height="756" class="img_ev3q"><figcaption>Fig 15. Medium mixed workload - Latency Test 1 - SSD</figcaption></figure><p></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="hdds-4">HDDs<a href="#hdds-4" class="hash-link" aria-label="Direct link to HDDs" title="Direct link to HDDs">​</a></h4>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 16. Medium mixed workload - Latency Test 1 - HDD" src="/rabbitmq-website/assets/images/mixed-medium-qq-b3-hdd-c153d5050a3933abd167c4b1650dec6c.png" width="910" height="752" class="img_ev3q"><figcaption>Fig 16. Medium mixed workload - Latency Test 1 - HDD</figcaption></figure><p></p>
<p>The single HDD basically was non functional, and the two and three disk configurations saw large latencies with the two disk one reaching above 1 second, despite the low message rate.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion-7">Conclusion<a href="#conclusion-7" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h4>
<p>HDD cluster end-to-end latency continues to worsen as both quorum queue and non-quorum queue load increases. SSDs remain largely unchanged.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="latency-benchmark-2---40-quorum-queues-2">Latency Benchmark #2 - 40 Quorum Queues<a href="#latency-benchmark-2---40-quorum-queues-2" class="hash-link" aria-label="Direct link to Latency Benchmark #2 - 40 Quorum Queues" title="Direct link to Latency Benchmark #2 - 40 Quorum Queues">​</a></h3>
<p>40 publishers, 40 queues, 40 consumers, 1kb messages, 50 msg/s per publisher (2000 msg/s total).</p>
<p>Lazy queue workload: 40 publishers, 40 queues, 40 consumers, 16b messages, 50 msg/s per publisher (2000 msg/s).</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="ssds-4">SSDs<a href="#ssds-4" class="hash-link" aria-label="Direct link to SSDs" title="Direct link to SSDs">​</a></h4>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 17. Medium mixed workload - Latency Test 2 - SSD" src="/rabbitmq-website/assets/images/mixed-medium-qq-b4-ssd-aaa0b01aa7ae451c9f720f0f53ccc527.png" width="911" height="752" class="img_ev3q"><figcaption>Fig 17. Medium mixed workload - Latency Test 2 - SSD</figcaption></figure><p></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="hdds-5">HDDs<a href="#hdds-5" class="hash-link" aria-label="Direct link to HDDs" title="Direct link to HDDs">​</a></h4>
<p></p><figure><img decoding="async" loading="lazy" alt="Fig 18. Medium mixed workload - Latency Test 2 - HDD" src="/rabbitmq-website/assets/images/mixed-medium-qq-b4-hdd-60c2c5d4a3b66be2e74e927258aca266.png" width="909" height="753" class="img_ev3q"><figcaption>Fig 18. Medium mixed workload - Latency Test 2 - HDD</figcaption></figure><p></p>
<p>Again, the single HDD cluster didn’t manage to handle any messages. The three disk configuration, which completely isolates quorum queue load from the lazy queue disk load saw the best latency, but still a much higher latency than the SSDs.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="final-conclusions">Final Conclusions<a href="#final-conclusions" class="hash-link" aria-label="Direct link to Final Conclusions" title="Direct link to Final Conclusions">​</a></h2>
<p>Quorum queues on SSDs show they are not overly sensitive to mixed workloads but can benefit from a multi-SSD drive configuration under higher loads.</p>
<p>We have also seen that quorum queues do not handle <strong>mixed</strong> workloads very well when running on HDDs. It is possible to mitigate those issues by isolating the segment and WAL files on separate HDDs from the mnesia (meta-data) and message store (classic queue data) workloads, but at a certain classic queue traffic level, the throughput will drop a lot. What that level of load is totally depends on your particular setup.</p>
<p>When might quorum queues be safe on HDDs? While not recommended you may still get good performance with low queue count pure quorum queue workloads or with mixed workloads which are low volume. But we have seen that quorum queue performance can drop precipitously on HDDs so you are taking a risk. We highly recommend SSDs and discourage the use of HDDs when employing quorum queues.</p>
<p>In the next post of this series we&#x27;ll look at migrating from classic mirrored queues to quorum queues, using a few example workloads to demonstrate what you might expect.</p></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/rabbitmq-website/blog/tags/performance">Performance</a></li></ul></div></div><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><a href="https://github.com/rabbitmq/rabbitmq-website/tree/main/blog/2020-04-21-quorum-queues-and-why-disks-matter/index.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/rabbitmq-website/blog/2020/05/04/quorum-queues-and-flow-control-the-concepts"><div class="pagination-nav__sublabel">Newer post</div><div class="pagination-nav__label">Quorum Queues and Flow Control - The Concepts</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/rabbitmq-website/blog/2020/04/20/rabbitmq-gets-an-ha-upgrade"><div class="pagination-nav__sublabel">Older post</div><div class="pagination-nav__label">RabbitMQ Gets an HA Upgrade</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#hdd-or-ssd-one-drive-or-multiple-drives" class="table-of-contents__link toc-highlight">HDD or SSD? One drive or multiple drives?</a></li><li><a href="#why-are-quorum-queues-sensitive-to-io-latency" class="table-of-contents__link toc-highlight">Why are quorum queues sensitive to IO latency?</a></li><li><a href="#ok-but-what-does-performance-with-hdds-look-like" class="table-of-contents__link toc-highlight">Ok, but what does performance with HDDs look like?</a></li><li><a href="#pure-quorum-queue-workload" class="table-of-contents__link toc-highlight">Pure Quorum Queue Workload</a><ul><li><a href="#throughput-benchmark-1---one-quorum-queue" class="table-of-contents__link toc-highlight">Throughput Benchmark #1 - One Quorum Queue</a></li><li><a href="#throughput-benchmark-2---four-quorum-queues" class="table-of-contents__link toc-highlight">Throughput Benchmark #2 - Four Quorum Queues</a></li><li><a href="#latency-benchmark-1---40-quorum-queues" class="table-of-contents__link toc-highlight">Latency Benchmark #1 - 40 Quorum Queues</a></li><li><a href="#latency-benchmark-2---40-quorum-queues" class="table-of-contents__link toc-highlight">Latency Benchmark #2 - 40 Quorum Queues</a></li></ul></li><li><a href="#light-mixed-workload-classic-lazy-and-quorum" class="table-of-contents__link toc-highlight">Light Mixed Workload (Classic Lazy and Quorum)</a><ul><li><a href="#throughput-benchmark-1---one-quorum-queue-1" class="table-of-contents__link toc-highlight">Throughput Benchmark #1 - One Quorum Queue</a></li><li><a href="#throughput-benchmark-2--four-quorum-queues" class="table-of-contents__link toc-highlight">Throughput Benchmark #2 - Four Quorum Queues</a></li><li><a href="#latency-benchmark-1---40-quorum-queues-1" class="table-of-contents__link toc-highlight">Latency Benchmark #1 - 40 Quorum Queues</a></li><li><a href="#latency-benchmark-2---40-quorum-queues-1" class="table-of-contents__link toc-highlight">Latency Benchmark #2 - 40 Quorum Queues</a></li></ul></li><li><a href="#medium-mixed-workload-classic-lazy-and-quorum" class="table-of-contents__link toc-highlight">Medium Mixed Workload (Classic Lazy and Quorum)</a><ul><li><a href="#throughput-benchmark-1---one-quorum-queue-2" class="table-of-contents__link toc-highlight">Throughput Benchmark #1 - One Quorum Queue</a></li><li><a href="#throughput-benchmark-2---four-quorum-queues-1" class="table-of-contents__link toc-highlight">Throughput Benchmark #2 - Four Quorum Queues</a></li><li><a href="#latency-benchmark-1---40-quorum-queues-2" class="table-of-contents__link toc-highlight">Latency Benchmark #1 - 40 Quorum Queues</a></li><li><a href="#latency-benchmark-2---40-quorum-queues-2" class="table-of-contents__link toc-highlight">Latency Benchmark #2 - 40 Quorum Queues</a></li></ul></li><li><a href="#final-conclusions" class="table-of-contents__link toc-highlight">Final Conclusions</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Learn about RabbitMQ</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/rabbitmq-website/tutorials">Getting Started</a></li><li class="footer__item"><a class="footer__link-item" href="/rabbitmq-website/docs">Documentation</a></li><li class="footer__item"><a class="footer__link-item" href="/rabbitmq-website/blog">Blog</a></li></ul></div><div class="col footer__col"><div class="footer__title">Reach out to the RabbitMQ team</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/rabbitmq" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/rabbitmq/rabbitmq-server/discussions" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub Discussions<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a class="footer__link-item" href="/rabbitmq-website/contact?utm_source=rmq_release-information_tableheader&amp;utm_medium=rmq_website&amp;utm_campaign=tanzu">Long Term Commercial Support</a></li><li class="footer__item"><a class="footer__link-item" href="/rabbitmq-website/contact">Contact Us</a></li><li class="footer__item"><a href="https://www.rabbitmq.com/discord" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Broadcom</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://tanzu.vmware.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">VMware Tanzu<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.vmware.com/help/legal.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">Terms of Use<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.vmware.com/help/privacy.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">Privacy<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a class="footer__link-item" href="/rabbitmq-website/trademark-guidelines">Trademark Guidelines</a></li><li class="footer__item"><a href="https://www.vmware.com/help/privacy/california-privacy-rights.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">Your California Privacy Rights<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a class="footer__link-item ot-sdk-show-settings">Cookie Settings</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2005-2025 Broadcom. All Rights Reserved. The term "Broadcom" refers to Broadcom Inc. and/or its subsidiaries.</div></div></div></footer></div>
</body>
</html>